{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-02T02:02:23.429516Z",
     "start_time": "2024-07-02T02:02:23.402986Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import networkx as nx\n",
    "from scipy.stats import truncnorm\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "import time as time_module\n",
    "def cal_time(found_node, store_node):\n",
    "    time4one_query = []\n",
    "    refuse_cnt = 0\n",
    "    accept_cnt = 0\n",
    "    time_consumed_all = 0 # 最终该query消耗的总计时间\n",
    "    for query in found_node:\n",
    "        query_nodup = list(set(query))\n",
    "        for node in query_nodup:\n",
    "            flag = False\n",
    "            for node_id, node_info in store_node.items():\n",
    "                if flag:\n",
    "                    break\n",
    "                if node == node_id:\n",
    "                    flag = True\n",
    "                    # 首先获取当前节点的概率值， 判断是否能够命中\n",
    "                    get_pro = node_info['probability']\n",
    "                    if probabilistic_true(get_pro):\n",
    "                        # 命中，计算时间开销\n",
    "                        time_consumed = node_info['distance'] * 0.010756\n",
    "                        #print(time_consumed)\n",
    "                        time4one_query.append(time_consumed)\n",
    "                        accept_cnt += 1\n",
    "                    else:\n",
    "                        # 未命中，记一次无服务次数\n",
    "                        refuse_cnt += 1\n",
    "        time_consumed_max = 0 # 最终该query消耗的总计时间\n",
    "        for time_consumed in time4one_query:\n",
    "            if time_consumed > time_consumed_max:\n",
    "                time_consumed_max = time_consumed\n",
    "        time4one_query.clear()\n",
    "        time_consumed_all += time_consumed_max\n",
    "    serve_prob = accept_cnt / (accept_cnt + refuse_cnt)\n",
    "    time_consumed_all = time_consumed_all * (1 + serve_prob)\n",
    "    return time_consumed_all, refuse_cnt, accept_cnt, serve_prob\n",
    "\n",
    "\n",
    "def probabilistic_true(probability):\n",
    "    return random.random() < probability\n",
    "def init_edge_weights(G):\n",
    "    count = 0\n",
    "    zero_count = 0\n",
    "    # 必须加上data=True才可以迭代\n",
    "    for u, u_attrs in G.nodes(data=True):\n",
    "        for v, v_attrs in G.nodes(data=True):\n",
    "            if u != v:\n",
    "                weight = 1 / (math.sqrt((u_attrs['device_id'] - v_attrs['device_id']) ** 2))\n",
    "                # 添加边到图中，并附带权重信息\n",
    "                G.add_edge(u, v, weight=weight)\n",
    "                count += 1\n",
    "    print(count)\n",
    "    print(zero_count)\n",
    "    print(\"Number of nodes:\", G.number_of_nodes())\n",
    "    print(\"Number of edges:\", G.number_of_edges())\n",
    "\n",
    "# 定义截断正态分布\n",
    "def truncated_normal(mean, std_dev, low, high):\n",
    "    std_dev = max(std_dev, 1e-6)  # 避免标准差为零\n",
    "    a = (low - mean) / std_dev\n",
    "    b = (high - mean) / std_dev\n",
    "    return truncnorm(a, b, loc=mean, scale=std_dev)\n",
    "\n",
    "\n",
    "def generateQuery(num_queries, time_low, time_high, device_value_max):\n",
    "    device_low, device_high = 1, device_value_max\n",
    "    query_time_length_mu = ((1 + time_on_chain) / 6)  # 尽可能让 time_length 较长，\n",
    "    query_time_length_sigma = max(int((time_on_chain - 1) / 6), 1e-6)\n",
    "\n",
    "    queries = []\n",
    "\n",
    "    def is_overlap_points_less_enough(time_lower_bound, time_upper_bound, device_lower_bound, device_upper_bound,\n",
    "                                      queries, min_points):\n",
    "        for (q_t_l, q_t_u), (q_d_l, q_d_u) in queries:\n",
    "            if time_lower_bound >= q_t_u or time_upper_bound <= q_t_l or device_upper_bound <= q_d_l or device_lower_bound >= q_d_u:\n",
    "                continue\n",
    "            else:\n",
    "                time_delta = min(abs(time_upper_bound - q_t_l), abs(time_upper_bound - time_lower_bound),\n",
    "                                 abs(time_lower_bound - q_t_u), abs(q_t_l - q_t_u))\n",
    "                device_delta = min(abs(device_upper_bound - q_d_l), abs(device_upper_bound - q_d_u),\n",
    "                                   abs(device_upper_bound - device_lower_bound), abs(q_d_l - q_d_u))\n",
    "                if (time_delta + 1) * (device_delta + 1) > min_points:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    for _ in range(num_queries):\n",
    "        bounds = []\n",
    "        while (True):\n",
    "            tried = 0\n",
    "            mean = (time_low + time_high) / 2\n",
    "            std_dev = max((time_high - time_low) / 6, 1e-6)  # 避免标准差为零, 经验法则：99.7%的数据在3个标准差内\n",
    "            # 生成time_center\n",
    "            time_center = round(truncated_normal(mean, std_dev, time_low, time_high).rvs())\n",
    "            # print(f\"time_center: {time_center}\")\n",
    "            time_length = abs(np.random.normal(query_time_length_mu, query_time_length_sigma))\n",
    "            # print(f\"time_length: {time_length}\")\n",
    "            time_lower_bound = round(max(time_low, time_center - time_length / 2))\n",
    "            time_upper_bound = round(min(time_high, time_center + time_length / 2))\n",
    "\n",
    "            mean = (device_low + device_high) / 2\n",
    "            std_dev = max((device_high - device_low) / 6, 1e-6)  # 避免标准差为零 经验法则：99.7%的数据在3个标准差内\n",
    "            # 生成time_center\n",
    "            device_center = round(truncated_normal(mean, std_dev, device_low, device_high).rvs())\n",
    "            # device_length = abs(np.random.normal(query_device_length_mu, query_device_length_sigma))\n",
    "            device_length = round(unit_num * batch_num / time_length)\n",
    "            device_lower_bound = round(max(device_low, device_center - device_length / 2))\n",
    "            device_upper_bound = round(min(device_high, device_center + device_length / 2))\n",
    "\n",
    "            area = (time_upper_bound - time_lower_bound + 1) * (device_upper_bound - device_lower_bound + 1)\n",
    "            tried += 1\n",
    "            # 一批数据的总数据点个数为400个, 共16个batch，得到每个batch中有25个数据点\n",
    "            # 如果生成query做到对数据点全覆盖，会导致2种batch方式最终都要访问所有的batch，时间开销上差距不大，故生成16 / 2 = 8个数据点比较合适\n",
    "            # 但是query也不能太少，因为query太少会导致很多节点没有出现在对图的权重进行迭代的过程中\n",
    "            if 1 <= area <= 30 and time_length >= 1 and is_overlap_points_less_enough(time_lower_bound,\n",
    "                                                                                      time_upper_bound,\n",
    "                                                                                      device_lower_bound,\n",
    "                                                                                      device_upper_bound, queries, 20):\n",
    "                bounds = [(time_lower_bound, time_upper_bound), (device_lower_bound, device_upper_bound)]\n",
    "                break\n",
    "            if tried == 10:\n",
    "                print(f\"try {tried} times.\")\n",
    "        queries.append(bounds)\n",
    "    return queries\n",
    "\n",
    "def calLaplacianMatrix(adjacentMatrix):\n",
    "    # compute the Degree Matrix: D=sum(A)\n",
    "    degreeMatrix = np.sum(adjacentMatrix, axis=1)\n",
    "    # compute the Laplacian Matrix: L=D-A\n",
    "    laplacianMatrix = np.diag(degreeMatrix) - adjacentMatrix\n",
    "    # print laplacianMatrix\n",
    "    # normalize\n",
    "    # D^(-1/2) L D^(-1/2)\n",
    "    sqrtDegreeMatrix = np.diag(1.0 / (degreeMatrix ** (0.5)))\n",
    "    return np.dot(np.dot(sqrtDegreeMatrix, laplacianMatrix), sqrtDegreeMatrix)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "rf_path = \"./rt-ifttt/rt-ifttt.csv\"\n",
    "rf_data = pd.read_csv(rf_path)\n",
    "\n",
    "weather_humidity_path = \"./Historical Hourly Weather Data 2012-2017/humidity.csv\"\n",
    "weather_pressure_path = \"./Historical Hourly Weather Data 2012-2017/pressure.csv\"\n",
    "weather_temperature_path = \"./Historical Hourly Weather Data 2012-2017/temperature.csv\"\n",
    "weather_wind_speed_path = \"./Historical Hourly Weather Data 2012-2017/wind_speed.csv\"\n",
    "weather_humidity_data = pd.read_csv(weather_humidity_path)\n",
    "weather_pressure_data = pd.read_csv(weather_pressure_path)\n",
    "weather_temperature_data = pd.read_csv(weather_temperature_path)\n",
    "weather_wind_speed_data = pd.read_csv(weather_wind_speed_path)\n",
    "weather_data = [weather_humidity_data, weather_pressure_data, weather_temperature_data, weather_wind_speed_data]\n",
    "\n",
    "\n",
    "for dataset in weather_data:\n",
    "    for col_name in dataset.columns:\n",
    "        # 处理空值\n",
    "        if dataset[col_name].isnull().any():\n",
    "            # 用平均值填充\n",
    "            dataset.fillna({col_name: int(dataset[col_name].mean())}, inplace=True)\n",
    "bridge_21416_DPM_path = \"./2021-04-16/2021-04-16 00-DPM.csv\"\n",
    "bridge_21416_HPT_path = \"./2021-04-16/2021-04-16 00-HPT.csv\"\n",
    "bridge_21416_RHS_path = \"./2021-04-16/2021-04-16 00-RHS.csv\"\n",
    "bridge_21416_ULT_path = \"./2021-04-16/2021-04-16 00-ULT.csv\"\n",
    "bridge_21416_VIB_path = \"./2021-04-16/2021-04-16 00-VIB.csv\"\n",
    "bridge_21416_VIC_path = \"./2021-04-16/2021-04-16 00-VIC.csv\"\n",
    "bridge_21416_DPM_data = pd.read_csv(bridge_21416_DPM_path)\n",
    "bridge_21416_HPT_data = pd.read_csv(bridge_21416_HPT_path)\n",
    "bridge_21416_RHS_data = pd.read_csv(bridge_21416_RHS_path)\n",
    "bridge_21416_ULT_data = pd.read_csv(bridge_21416_ULT_path)\n",
    "bridge_21416_VIB_data = pd.read_csv(bridge_21416_VIB_path)\n",
    "bridge_21416_VIC_data = pd.read_csv(bridge_21416_VIC_path)\n",
    "\n",
    "bridge_data = [bridge_21416_DPM_data, bridge_21416_HPT_data, bridge_21416_RHS_data, bridge_21416_ULT_data,\n",
    "               bridge_21416_VIB_data, bridge_21416_VIC_data]\n",
    "\n",
    "for idx in range(len(weather_data)):\n",
    "    weather_data[idx] = weather_data[idx].drop(weather_data[idx].columns[0], axis=1)\n",
    "\n",
    "weather_data_list = [\"_humidity\", \"_pressure\", \"_temperature\", \"_wind_speed\"]\n",
    "for idx_dataset in range(len(weather_data)):\n",
    "    for idx in range(len(weather_data[idx_dataset].columns)):\n",
    "        weather_data[idx_dataset].columns.values[idx] += weather_data_list[idx_dataset]\n",
    "\n",
    "data_aggregated = pd.concat([rf_data], axis=1)\n",
    "data_aggregated.columns.values[0] = \"Timestamp\"\n",
    "for idx in range(len(weather_data)):\n",
    "    data_aggregated = pd.concat([data_aggregated, weather_data[idx].head(10000)], axis=1)\n",
    "\n",
    "# 模拟存储节点\n",
    "# 每个store_node 包含3个信息，distance|storage space|reputation, 并计算得到一个score\n",
    "store_node_num = 50\n",
    "store_node = {i: {'distance': 0, 'storage space': 0, 'probability': 0, 'score_sp': 0, 'score_sd': 0, 'batches': []}\n",
    "              for i in range(1, store_node_num + 1)}\n",
    "\n",
    "mean, std_dev = 4000, 2200  # 均值, 标准差\n",
    "gaussian_distances = np.random.normal(mean, std_dev, store_node_num)  # 生成 distance\n",
    "mean, std_dev = 1000, 160  # 均值, 标准差\n",
    "gaussian_storage_spaces = np.random.normal(mean, std_dev, store_node_num)  # 生成 storage_spaces\n",
    "mean, std_dev = 0.6, 0.1  # 均值, 标准差\n",
    "gaussian_probability = np.random.normal(mean, std_dev, store_node_num)  # 生成 storage_spaces\n",
    "gaussian_probability = np.clip(gaussian_probability, 0, 1)\n",
    "\n",
    "alpha_probability = 1000\n",
    "alpha_distance = 10\n",
    "# 循环遍历每个节点，为其设置高斯分布的距离\n",
    "for key, node in store_node.items():\n",
    "    node['distance'] = gaussian_distances[key - 1]\n",
    "    node['storage space'] = gaussian_storage_spaces[key - 1]\n",
    "    node['probability'] = gaussian_probability[key - 1]\n",
    "    # node['score_sp'] = node['storage space'] + alpha_probability * node['probability']\n",
    "    node['score_sp'] = alpha_probability * node['probability']\n",
    "    # node['score_sd'] = node['storage space'] - alpha_distance * node['distance']\n",
    "    node['score_sd'] = node['distance']\n",
    "\n",
    "# Settings\n",
    "\n",
    "device_value_max = 20\n",
    "time_on_chain = 30\n",
    "time_max = 299\n",
    "# 遍历各种 unit_num与 batch_num 组合得到各组实验数据\n",
    "# 每个 unit  包含的 元素 数量\n",
    "# 每个 batch 包含的 unit 数量\n",
    "# (unit_num, batch_num)\n",
    "unit_batch_num_list = [(5, 2), (5, 4), (5, 6), (5, 8), (5, 10), (5, 12), (5, 20)]\n",
    "\n",
    "results = {\n",
    "    '1': [],\n",
    "    '10': [],\n",
    "    '20': [],\n",
    "    '30': [],\n",
    "    '40': [],\n",
    "    '50': [],\n",
    "    '60': [],\n",
    "    '70': [],\n",
    "    '80': [],\n",
    "    '90': [],\n",
    "    '100': [],\n",
    "}\n",
    "\n",
    "unit_num, batch_num = 1, 1\n",
    "print(f\"unit_num: {unit_num} batch_num: {batch_num} batch size: {unit_num * batch_num}\")\n",
    "batched_basic_method = {}\n",
    "cnt = 0\n",
    "for start_time in range(0, time_max, time_on_chain):\n",
    "    end_time = start_time + time_on_chain\n",
    "    batch_list = []\n",
    "    for time in range(start_time, end_time):\n",
    "        for device in range(1, device_value_max + 1):\n",
    "            # data_points.append(data_aggregated.loc[time][device])\n",
    "            batch_list.append([[(time, device)]])  # 400 个数据点\n",
    "    random.shuffle(batch_list)  # 随机打乱\n",
    "\n",
    "    for batch in batch_list:\n",
    "        batched_basic_method[str(cnt)] = batch\n",
    "        cnt += 1\n",
    "\n",
    "batched_proposed_method = {}  # 以字典存储所有的Batch batch_id : []\n",
    "cnt = 0\n",
    "Query_sets = []\n",
    "for start_time in range(0, time_max, time_on_chain):\n",
    "    end_time = start_time + time_on_chain\n",
    "    units_list = []\n",
    "    unit_tmp = []\n",
    "    device_id = 1\n",
    "    while device_id <= device_value_max:\n",
    "        time_cnt = 0\n",
    "        for time in range(start_time, end_time):\n",
    "            # data_points.append(data_aggregated.loc[time][device])\n",
    "            unit_tmp.append((time, device_id))\n",
    "            time_cnt += 1\n",
    "            if time_cnt % unit_num == 0:\n",
    "                units_list.append((device_id, copy.deepcopy(unit_tmp)))\n",
    "                unit_tmp.clear()\n",
    "        device_id += 1\n",
    "\n",
    "    num_queries = 10\n",
    "    # num_queries = int(time_on_chain * device_value_max / unit_num / batch_num / 2) # 16 / 2\n",
    "    # 本次生成的Query用于更新下一次的Device\n",
    "    query_set = generateQuery(num_queries, start_time, end_time - 1, device_value_max)\n",
    "    Query_sets.append(copy.deepcopy(query_set))  # 将每段on_chain时间内生成的query_set整合到循环外部一个总的Query集合中\n",
    "\n",
    "    # 更新Device, 从第二批数据开始，按照query更新\n",
    "    batch_list = []\n",
    "    for (device_id, unit) in units_list:\n",
    "        batch_list.append(copy.deepcopy(unit))\n",
    "\n",
    "    for batch in batch_list:\n",
    "        batched_proposed_method[str(cnt)] = batch\n",
    "        cnt += 1\n",
    "\n",
    "store_node_method_r_d = copy.deepcopy(store_node)\n",
    "# batched_basic_method\n",
    "store_node_method_r_d_batched_basic_method = copy.deepcopy(store_node_method_r_d)\n",
    "\n",
    "for key, batch in batched_basic_method.items():\n",
    "    # 要选取一个store node存储\n",
    "    sorted_nodes_r_d = sorted(store_node_method_r_d_batched_basic_method.items(), key=lambda x: x[1]['score_sd'],\n",
    "                              reverse=False)\n",
    "    # 从得分前 10 的节点中随机选择一个节点\n",
    "    top_ten_nodes_r_d = sorted_nodes_r_d[:10]\n",
    "    saved_flag = False\n",
    "    for node in top_ten_nodes_r_d:\n",
    "        if saved_flag:\n",
    "            break\n",
    "        get_pro = node[1]['probability']\n",
    "        # 得到命中概率\n",
    "        if probabilistic_true(get_pro):\n",
    "            # 若命中，则存入\n",
    "            store_node_method_r_d_batched_basic_method[node[0]]['batches'].append((key, batch))\n",
    "            saved_flag = True\n",
    "    if not saved_flag:\n",
    "        # 10 个都没命中，则存入 rank 第一的 node 中\n",
    "        store_node_method_r_d_batched_basic_method[top_ten_nodes_r_d[0][0]]['batches'].append((key, batch))\n",
    "# batched_proposed_method\n",
    "store_node_method_r_d_batched_proposed_method = copy.deepcopy(store_node_method_r_d)\n",
    "\n",
    "for key, batch in batched_proposed_method.items():\n",
    "    # 要选取一个store node存储\n",
    "    sorted_nodes = sorted(store_node_method_r_d_batched_proposed_method.items(), key=lambda x: x[1]['score_sd'],\n",
    "                          reverse=False)  # 考虑 Distance 升序，距离小的优先\n",
    "    # 从得分前 10 的节点中随机选择一个节点\n",
    "    top_ten_nodes = sorted_nodes[:10]\n",
    "    saved_flag = False\n",
    "    for node in top_ten_nodes:\n",
    "        if saved_flag:\n",
    "            break\n",
    "        get_pro = node[1]['probability']\n",
    "        # 得到命中概率\n",
    "        if probabilistic_true(get_pro):\n",
    "            # 若命中，则存入\n",
    "            store_node_method_r_d_batched_proposed_method[node[0]]['batches'].append((key, batch))  # 考虑 reputation\n",
    "            saved_flag = True\n",
    "    if not saved_flag:\n",
    "        # 10 个都没命中，则存入 rank 第一的 node 中\n",
    "        store_node_method_r_d_batched_proposed_method[top_ten_nodes[0][0]]['batches'].append((key, batch))\n",
    "\n",
    "# 遍历Query_sets，在两种store node的存储方式上，访问所有的query，计算得到指标\n",
    "found_in_basic_batch = []  # [[], [], ..., []] 记录了每个 query 中每个 point 所在的batch_id ， 用于后续去 store\n",
    "found_in_proposed_batch = []\n",
    "for query_set in Query_sets:\n",
    "    basic_batches4q = []\n",
    "    proposed_batches4q = []\n",
    "    for query in query_set:\n",
    "        # 对每一个 q, 得到 time 和 device\n",
    "        time_low, time_high, device_low, device_high = query[0][0], query[0][1], query[1][0], query[1][1]\n",
    "        # print(f\"time_low:{time_low}, time_high:{time_high}, device_low:{device_low}, device_high:{device_high}\")\n",
    "        # 找到 device 所在的 batch, 计算出每个 q 需要访问哪些 batch\n",
    "        # 直接遍历搜索空间过大，进行优化: 从time来确定Batch_id范围: 0~19s 内的数据处于 0~3 的 batch 之间\n",
    "        # 由于 同一批Query_set中的元素都在同一批Batch中，故直接由time_low定出 batch range\n",
    "        batch_low, batch_high = int(time_low / time_on_chain) * int(\n",
    "            time_on_chain * device_value_max / unit_num / batch_num), (int(time_low / time_on_chain) + 1) * int(\n",
    "            time_on_chain * device_value_max / unit_num / batch_num) - 1\n",
    "        # print(f\"batch_low:{batch_low} and batch_high:{batch_high}\")\n",
    "        for device in range(device_low, device_high + 1):\n",
    "            for time in range(time_low, time_high + 1):\n",
    "                # print(f\"time: {time}, device: {device}\")\n",
    "                basic_flag = False\n",
    "                proposed_flag = False\n",
    "                # 对每个 （time, device） 找到所属的Batch\n",
    "                for batch_id in range(batch_low, batch_high + 1):\n",
    "                    # print(f\"batch id is : {batch}\")\n",
    "                    # batch结构: [[], [], ..., []]\n",
    "                    if not basic_flag:\n",
    "                        batch = copy.deepcopy(batched_basic_method[str(batch_id)])\n",
    "                        for unit in batch:\n",
    "                            for point in unit:\n",
    "                                if time == point[0] and device == point[1]:\n",
    "                                    # print(f\"----basic find point\")\n",
    "                                    # 找到了所属的Batch，继续寻找所属的store node\n",
    "                                    basic_batches4q.append(batch_id)\n",
    "                                    basic_flag = True\n",
    "\n",
    "                    if not proposed_flag:\n",
    "                        batch = copy.deepcopy(batched_proposed_method[str(batch_id)])\n",
    "                        for unit in batch:\n",
    "\n",
    "                            if time == unit[0] and device == unit[1]:\n",
    "                                # print(f\"----proposed find point\")\n",
    "                                # 找到了所属的Batch，继续寻找所属的store node\n",
    "                                proposed_batches4q.append(batch_id)\n",
    "                                proposed_flag = True\n",
    "                    if proposed_flag and basic_flag:\n",
    "                        break\n",
    "                # print(f\"belong_to_basic_batch:{belong_to_basic_batch} and belong_to_proposed_batch:{belong_to_proposed_batch}\")\n",
    "        found_in_basic_batch.append(copy.deepcopy(basic_batches4q))\n",
    "        found_in_proposed_batch.append(copy.deepcopy(proposed_batches4q))\n",
    "        basic_batches4q.clear()\n",
    "        proposed_batches4q.clear()\n",
    "\n",
    "for idx in range(len(found_in_basic_batch)):\n",
    "    found_in_basic_batch[idx] = list(set(found_in_basic_batch[idx]))\n",
    "for idx in range(len(found_in_proposed_batch)):\n",
    "    found_in_proposed_batch[idx] = list(set(found_in_proposed_batch[idx]))\n",
    "\n",
    "batches_sum_basic = 0\n",
    "for item in found_in_basic_batch:\n",
    "    batches_sum_basic += len(item)\n",
    "print(f\"basic 的 batch 数量{batches_sum_basic}\")\n",
    "batches_sum_proposed = 0\n",
    "for item in found_in_proposed_batch:\n",
    "    batches_sum_proposed += len(item)\n",
    "print(f\"proposed 的 batch 数量{batches_sum_proposed}\")\n",
    "results[str(unit_num * batch_num)].append(0)\n",
    "results[str(unit_num * batch_num)].append((batches_sum_basic, batches_sum_proposed))\n",
    "# 现在已经找到了所有要访问的 Batch, 之后到store_node_method_r_d_batched_basic_method 和 store_node_method_r_d_batched_proposed_method 中寻找 存储了相应的 Batch 的 node\n",
    "# 要访问的每个 batch 在哪个 store node 中, 并计算累计的 1. 访问无服务次数 2. 有服务时的访问时延\n",
    "# 在Batch中 按照每5个Query为单位 存储了要访问的batch 有哪几个（会出现重复）\n",
    "\n",
    "# found_in_basic_batch = []      # [[], [], ..., []]\n",
    "# found_in_proposed_batch = []   # [[], [], ..., []]\n",
    "\n",
    "# found_in_basic_node\n",
    "found_in_basic_node_r_d = []\n",
    "basic_node_tmp = []\n",
    "for query in found_in_basic_batch:\n",
    "    # print(query)\n",
    "    found_batches = list(set(query))\n",
    "    # print(found_batches)\n",
    "    for batch_id in found_batches:\n",
    "        # 首先要合并所有相同的 batch_id , 再对每个不同的Batch_id到store_node中找到 node_id 存入found_in_xxx_node\n",
    "        flag = False\n",
    "        for node_id, node_info in store_node_method_r_d_batched_basic_method.items():\n",
    "            if flag:\n",
    "                break\n",
    "            for key, value in node_info['batches']:\n",
    "                if int(batch_id) == int(key):\n",
    "                    basic_node_tmp.append(node_id)\n",
    "                    flag = True\n",
    "                    break\n",
    "    found_in_basic_node_r_d.append(copy.deepcopy(basic_node_tmp))\n",
    "    basic_node_tmp.clear()\n",
    "\n",
    "# found_in_proposed_node\n",
    "found_in_proposed_node_r_d = []\n",
    "proposed_node_tmp = []\n",
    "for query in found_in_proposed_batch:\n",
    "    # print(query)\n",
    "    found_batches = list(set(query))\n",
    "    # print(found_batches)\n",
    "    for batch_id in found_batches:\n",
    "        # 首先要合并所有相同的 batch_id , 再对每个不同的Batch_id到store_node中找到 node_id 存入found_in_xxx_node\n",
    "        flag = False\n",
    "        for node_id, node_info in store_node_method_r_d_batched_proposed_method.items():\n",
    "            if flag:\n",
    "                break\n",
    "            for key, value in node_info['batches']:\n",
    "                if int(batch_id) == int(key):\n",
    "                    proposed_node_tmp.append(node_id)\n",
    "                    flag = True\n",
    "                    break\n",
    "    found_in_proposed_node_r_d.append(copy.deepcopy(proposed_node_tmp))\n",
    "    proposed_node_tmp.clear()\n",
    "for idx in range(len(found_in_basic_node_r_d)):\n",
    "    found_in_basic_node_r_d[idx] = list(set(found_in_basic_node_r_d[idx]))\n",
    "for idx in range(len(found_in_proposed_node_r_d)):\n",
    "    found_in_proposed_node_r_d[idx] = list(set(found_in_proposed_node_r_d[idx]))\n",
    "nodes_sum_basic = 0\n",
    "for item in found_in_basic_node_r_d:\n",
    "    nodes_sum_basic += len(item)\n",
    "print(f\"basic r_d 的 node 数量{nodes_sum_basic}\")\n",
    "nodes_sum_proposed = 0\n",
    "for item in found_in_proposed_node_r_d:\n",
    "    nodes_sum_proposed += len(item)\n",
    "print(f\"proposed r_d 的 node 数量{nodes_sum_proposed}\")\n",
    "results[str(unit_num * batch_num)].append((nodes_sum_basic, nodes_sum_proposed))\n",
    "time_consumed_max_basic, refuse_cnt_basic, accept_cnt_basic, serve_prob_basic = cal_time(found_in_basic_node_r_d,\n",
    "                                                                                         store_node_method_r_d_batched_basic_method)\n",
    "time_consumed_max_proposed, refuse_cnt_proposed, accept_cnt_proposed, serve_prob_proposed = cal_time(\n",
    "    found_in_proposed_node_r_d, store_node_method_r_d_batched_proposed_method)\n",
    "results[str(unit_num * batch_num)].append((time_consumed_max_basic, refuse_cnt_basic, accept_cnt_basic, serve_prob_basic))\n",
    "results[str(unit_num * batch_num)].append((time_consumed_max_proposed, refuse_cnt_proposed, accept_cnt_proposed, serve_prob_proposed))\n",
    "\n",
    "store_node_method_r = copy.deepcopy(store_node)\n",
    "store_node_method_r_batched_basic_method = copy.deepcopy(store_node_method_r)\n",
    "\n",
    "for key, batch in batched_basic_method.items():\n",
    "    # 要选取一个store node存储\n",
    "    sorted_nodes_r = sorted(store_node_method_r_batched_basic_method.items(), key=lambda x: x[1]['score_sp'],\n",
    "                            reverse=True)  # Ture 指定为降序排序\n",
    "    # 从得分前 10 的节点中随机选择一个节点\n",
    "    top_ten_nodes_r = sorted_nodes_r[:10]\n",
    "    saved_flag = False\n",
    "    for node in top_ten_nodes_r:\n",
    "        if saved_flag:\n",
    "            break\n",
    "        get_pro = node[1]['probability']\n",
    "        # 得到命中概率\n",
    "        if probabilistic_true(get_pro):\n",
    "            # 若命中，则存入\n",
    "            store_node_method_r_batched_basic_method[node[0]]['batches'].append((key, batch))\n",
    "            saved_flag = True\n",
    "    if not saved_flag:\n",
    "        # 10 个都没命中，则存入 rank 第一的 node 中\n",
    "        store_node_method_r_batched_basic_method[top_ten_nodes_r[0][0]]['batches'].append((key, batch))\n",
    "store_node_method_r_batched_proposed_method = copy.deepcopy(store_node_method_r)\n",
    "\n",
    "for key, batch in batched_proposed_method.items():\n",
    "    # 要选取一个store node存储\n",
    "    sorted_nodes_r = sorted(store_node_method_r_batched_proposed_method.items(), key=lambda x: x[1]['score_sp'],\n",
    "                            reverse=True)\n",
    "    # 从得分前 10 的节点中随机选择一个节点\n",
    "    top_ten_nodes_r = sorted_nodes_r[:10]\n",
    "    saved_flag = False\n",
    "    for node in top_ten_nodes_r:\n",
    "        if saved_flag:\n",
    "            break\n",
    "        get_pro = node[1]['probability']\n",
    "        # 得到命中概率\n",
    "        if probabilistic_true(get_pro):\n",
    "            # 若命中，则存入\n",
    "            store_node_method_r_batched_proposed_method[node[0]]['batches'].append((key, batch))\n",
    "            saved_flag = True\n",
    "    if not saved_flag:\n",
    "        # 10 个都没命中，则存入 rank 第一的 node 中\n",
    "        store_node_method_r_batched_proposed_method[top_ten_nodes_r[0][0]]['batches'].append((key, batch))\n",
    "\n",
    "# 遍历Query_sets，在两种store node的存储方式上，访问所有的query，计算得到指标\n",
    "found_in_basic_batch_r = []  # [[], [], ..., []] 记录了每个 query 中每个 point 所在的batch_id ， 用于后续去 store\n",
    "found_in_proposed_batch_r = []\n",
    "for query_set in Query_sets:\n",
    "    basic_batches4q = []\n",
    "    proposed_batches4q = []\n",
    "    for query in query_set:\n",
    "        # 对每一个 q, 得到 time 和 device\n",
    "        time_low, time_high, device_low, device_high = query[0][0], query[0][1], query[1][0], query[1][1]\n",
    "        # print(f\"time_low:{time_low}, time_high:{time_high}, device_low:{device_low}, device_high:{device_high}\")\n",
    "        # 找到 device 所在的 batch, 计算出每个 q 需要访问哪些 batch\n",
    "        # 直接遍历搜索空间过大，进行优化: 从time来确定Batch_id范围: 0~19s 内的数据处于 0~3 的 batch 之间\n",
    "        # 由于 同一批Query_set中的元素都在同一批Batch中，故直接由time_low定出 batch range\n",
    "        batch_low, batch_high = int(time_low / time_on_chain) * int(\n",
    "            time_on_chain * device_value_max / unit_num / batch_num), (int(time_low / time_on_chain) + 1) * int(\n",
    "            time_on_chain * device_value_max / unit_num / batch_num) - 1\n",
    "        # print(f\"batch_low:{batch_low} and batch_high:{batch_high}\")\n",
    "        for device in range(device_low, device_high + 1):\n",
    "            for time in range(time_low, time_high + 1):\n",
    "                # print(f\"time: {time}, device: {device}\")\n",
    "                basic_flag = False\n",
    "                proposed_flag = False\n",
    "                # 对每个 （time, device） 找到所属的Batch\n",
    "                for batch_id in range(batch_low, batch_high + 1):\n",
    "                    # print(f\"batch id is : {batch}\")\n",
    "                    # batch结构: [[], [], ..., []]\n",
    "                    if not basic_flag:\n",
    "                        batch = copy.deepcopy(batched_basic_method[str(batch_id)])\n",
    "                        for unit in batch:\n",
    "                            for point in unit:\n",
    "                                # print(f\"{point[0]}  and  {point[1]}\")\n",
    "                                if time == point[0] and device == point[1]:\n",
    "                                    # print(f\"----basic find point\")\n",
    "                                    # 找到了所属的Batch，继续寻找所属的store node\n",
    "                                    basic_batches4q.append(batch_id)\n",
    "                                    basic_flag = True\n",
    "                    if not proposed_flag:\n",
    "                        batch = copy.deepcopy(batched_proposed_method[str(batch_id)])\n",
    "                        for unit in batch:\n",
    "                            if time == unit[0] and device == unit[1]:\n",
    "                                # print(f\"----proposed find point\")\n",
    "                                # 找到了所属的Batch，继续寻找所属的store node\n",
    "                                proposed_batches4q.append(batch_id)\n",
    "                                proposed_flag = True\n",
    "                    if proposed_flag and basic_flag:\n",
    "                        break\n",
    "                # print(f\"belong_to_basic_batch:{belong_to_basic_batch} and belong_to_proposed_batch:{belong_to_proposed_batch}\")\n",
    "        found_in_basic_batch_r.append(copy.deepcopy(basic_batches4q))\n",
    "        found_in_proposed_batch_r.append(copy.deepcopy(proposed_batches4q))\n",
    "        basic_batches4q.clear()\n",
    "        proposed_batches4q.clear()\n",
    "found_in_basic_node_r = []\n",
    "basic_node_tmp = []\n",
    "for query in found_in_basic_batch_r:\n",
    "    # print(query)\n",
    "    found_batches = list(set(query))\n",
    "    # print(found_batches)\n",
    "    for batch_id in found_batches:\n",
    "        # 首先要合并所有相同的 batch_id , 再对每个不同的Batch_id到store_node中找到 node_id 存入found_in_xxx_node\n",
    "        flag = False\n",
    "        for node_id, node_info in store_node_method_r_batched_basic_method.items():\n",
    "            if flag:\n",
    "                break\n",
    "            for key, value in node_info['batches']:\n",
    "                if int(batch_id) == int(key):\n",
    "                    basic_node_tmp.append(node_id)\n",
    "                    flag = True\n",
    "                    break\n",
    "    found_in_basic_node_r.append(copy.deepcopy(basic_node_tmp))\n",
    "    basic_node_tmp.clear()\n",
    "\n",
    "# found_in_proposed_node\n",
    "found_in_proposed_node_r = []\n",
    "proposed_node_tmp = []\n",
    "for query in found_in_proposed_batch:\n",
    "    # print(query)\n",
    "    found_batches = list(set(query))\n",
    "    # print(found_batches)\n",
    "    for batch_id in found_batches:\n",
    "        # 首先要合并所有相同的 batch_id , 再对每个不同的Batch_id到store_node中找到 node_id 存入found_in_xxx_node\n",
    "        flag = False\n",
    "        for node_id, node_info in store_node_method_r_batched_proposed_method.items():\n",
    "            if flag:\n",
    "                break\n",
    "            for key, value in node_info['batches']:\n",
    "                if int(batch_id) == int(key):\n",
    "                    proposed_node_tmp.append(node_id)\n",
    "                    flag = True\n",
    "                    break\n",
    "    found_in_proposed_node_r.append(copy.deepcopy(proposed_node_tmp))\n",
    "    proposed_node_tmp.clear()\n",
    "\n",
    "for idx in range(len(found_in_basic_node_r)):\n",
    "    found_in_basic_node_r[idx] = list(set(found_in_basic_node_r[idx]))\n",
    "for idx in range(len(found_in_proposed_node_r)):\n",
    "    found_in_proposed_node_r[idx] = list(set(found_in_proposed_node_r[idx]))\n",
    "nodes_sum_basic = 0\n",
    "for item in found_in_basic_node_r:\n",
    "    nodes_sum_basic += len(item)\n",
    "print(f\"basic r 的 node 数量{nodes_sum_basic}\")\n",
    "nodes_sum_proposed = 0\n",
    "for item in found_in_proposed_node_r:\n",
    "    nodes_sum_proposed += len(item)\n",
    "print(f\"proposed r 的 node 数量{nodes_sum_proposed}\")\n",
    "results[str(unit_num * batch_num)].append((nodes_sum_basic, nodes_sum_proposed))\n",
    "\n",
    "time_consumed_max_basic, refuse_cnt_basic, accept_cnt_basic, serve_prob_basic = cal_time(found_in_basic_node_r,\n",
    "                                                                                         store_node_method_r_batched_basic_method)\n",
    "time_consumed_max_proposed, refuse_cnt_proposed, accept_cnt_proposed, serve_prob_proposed = cal_time(\n",
    "    found_in_proposed_node_r, store_node_method_r_batched_proposed_method)\n",
    "results[str(unit_num * batch_num)].append((time_consumed_max_basic, refuse_cnt_basic, accept_cnt_basic, serve_prob_basic))\n",
    "results[str(unit_num * batch_num)].append((time_consumed_max_proposed, refuse_cnt_proposed, accept_cnt_proposed, serve_prob_proposed))\n",
    "\n",
    "store_node_method_d = copy.deepcopy(store_node)\n",
    "# batched_basic_method\n",
    "store_node_method_d_batched_basic_method = copy.deepcopy(store_node_method_d)\n",
    "\n",
    "for key, batch in batched_basic_method.items():\n",
    "    # 要选取一个store node存储\n",
    "    sorted_nodes_d = sorted(store_node_method_d_batched_basic_method.items(), key=lambda x: x[1]['score_sd'],\n",
    "                            reverse=False)  # 升序排序，距离越小越优先\n",
    "    # 从得分前 10 的节点中随机选择一个节点\n",
    "    top_ten_nodes = sorted_nodes_d[:10]\n",
    "    random_choice_node = random.choice(top_ten_nodes)\n",
    "    store_node_method_d_batched_basic_method[random_choice_node[0]]['batches'].append((key, batch))\n",
    "\n",
    "store_node_method_d_batched_proposed_method = copy.deepcopy(store_node_method_d)\n",
    "\n",
    "for key, batch in batched_proposed_method.items():\n",
    "    # 要选取一个store node存储\n",
    "    sorted_nodes_d = sorted(store_node_method_d_batched_proposed_method.items(), key=lambda x: x[1]['score_sd'],\n",
    "                            reverse=False)  # 升序排序，距离越小越优先\n",
    "    # 从得分前 10 的节点中随机选择一个节点\n",
    "    top_ten_nodes = sorted_nodes_d[:10]\n",
    "    random_choice_node = random.choice(top_ten_nodes)\n",
    "    store_node_method_d_batched_proposed_method[random_choice_node[0]]['batches'].append((key, batch))\n",
    "\n",
    "# 遍历Query_sets，在两种store node的存储方式上，访问所有的query，计算得到指标\n",
    "found_in_basic_batch = []  # [[], [], ..., []] 记录了每个 query 中每个 point 所在的batch_id ， 用于后续去 store\n",
    "found_in_proposed_batch = []\n",
    "found_in_basic_batch.clear()\n",
    "found_in_proposed_batch.clear()\n",
    "for query_set in Query_sets:\n",
    "    basic_batches4q = []\n",
    "    proposed_batches4q = []\n",
    "    for query in query_set:\n",
    "        # 对每一个 q, 得到 time 和 device\n",
    "        time_low, time_high, device_low, device_high = query[0][0], query[0][1], query[1][0], query[1][1]\n",
    "        # print(f\"time_low:{time_low}, time_high:{time_high}, device_low:{device_low}, device_high:{device_high}\")\n",
    "        # 找到 device 所在的 batch, 计算出每个 q 需要访问哪些 batch\n",
    "        # 直接遍历搜索空间过大，进行优化: 从time来确定Batch_id范围: 0~19s 内的数据处于 0~3 的 batch 之间\n",
    "        # 由于 同一批Query_set中的元素都在同一批Batch中，故直接由time_low定出 batch range\n",
    "        batch_low, batch_high = int(time_low / time_on_chain) * int(\n",
    "            time_on_chain * device_value_max / unit_num / batch_num), (int(time_low / time_on_chain) + 1) * int(\n",
    "            time_on_chain * device_value_max / unit_num / batch_num) - 1\n",
    "        # print(f\"batch_low:{batch_low} and batch_high:{batch_high}\")\n",
    "        for device in range(device_low, device_high + 1):\n",
    "            for time in range(time_low, time_high + 1):\n",
    "                # print(f\"time: {time}, device: {device}\")\n",
    "                basic_flag = False\n",
    "                proposed_flag = False\n",
    "                # 对每个 （time, device） 找到所属的Batch\n",
    "                for batch_id in range(batch_low, batch_high + 1):\n",
    "                    # print(f\"batch id is : {batch}\")\n",
    "                    # batch结构: [[], [], ..., []]\n",
    "\n",
    "                    if not basic_flag:\n",
    "                        batch = copy.deepcopy(batched_basic_method[str(batch_id)])\n",
    "                        for unit in batch:\n",
    "                            for point in unit:\n",
    "                                # print(f\"{point[0]}  and  {point[1]}\")\n",
    "                                if time == point[0] and device == point[1]:\n",
    "                                    # print(f\"----basic find point\")\n",
    "                                    # 找到了所属的Batch，继续寻找所属的store node\n",
    "                                    basic_batches4q.append(batch_id)\n",
    "                                    basic_flag = True\n",
    "\n",
    "                    if not proposed_flag:\n",
    "                        batch = copy.deepcopy(batched_proposed_method[str(batch_id)])\n",
    "                        for unit in batch:\n",
    "                            if time == unit[0] and device == unit[1]:\n",
    "                                # print(f\"----proposed find point\")\n",
    "                                # 找到了所属的Batch，继续寻找所属的store node\n",
    "                                proposed_batches4q.append(batch_id)\n",
    "                                proposed_flag = True\n",
    "                    if proposed_flag and basic_flag:\n",
    "                        break\n",
    "                # print(f\"belong_to_basic_batch:{belong_to_basic_batch} and belong_to_proposed_batch:{belong_to_proposed_batch}\")\n",
    "        found_in_basic_batch.append(copy.deepcopy(basic_batches4q))\n",
    "        found_in_proposed_batch.append(copy.deepcopy(proposed_batches4q))\n",
    "        basic_batches4q.clear()\n",
    "        proposed_batches4q.clear()\n",
    "found_in_basic_node_d = []\n",
    "basic_node_tmp = []\n",
    "for query in found_in_basic_batch:\n",
    "    # print(query)\n",
    "    found_batches = list(set(query))\n",
    "    # print(found_batches)\n",
    "    for batch_id in found_batches:\n",
    "        # 首先要合并所有相同的 batch_id , 再对每个不同的Batch_id到store_node中找到 node_id 存入found_in_xxx_node\n",
    "        flag = False\n",
    "        for node_id, node_info in store_node_method_d_batched_basic_method.items():\n",
    "            if flag:\n",
    "                break\n",
    "            for key, value in node_info['batches']:\n",
    "                if int(batch_id) == int(key):\n",
    "                    basic_node_tmp.append(node_id)\n",
    "                    flag = True\n",
    "                    break\n",
    "    found_in_basic_node_d.append(copy.deepcopy(basic_node_tmp))\n",
    "    basic_node_tmp.clear()\n",
    "\n",
    "# found_in_proposed_node\n",
    "found_in_proposed_node_d = []\n",
    "proposed_node_tmp = []\n",
    "for query in found_in_proposed_batch:\n",
    "    # print(query)\n",
    "    found_batches = list(set(query))\n",
    "    # print(found_batches)\n",
    "    for batch_id in found_batches:\n",
    "        # 首先要合并所有相同的 batch_id , 再对每个不同的Batch_id到store_node中找到 node_id 存入found_in_xxx_node\n",
    "        flag = False\n",
    "        for node_id, node_info in store_node_method_d_batched_proposed_method.items():\n",
    "            if flag:\n",
    "                break\n",
    "            for key, value in node_info['batches']:\n",
    "                if int(batch_id) == int(key):\n",
    "                    proposed_node_tmp.append(node_id)\n",
    "                    flag = True\n",
    "                    break\n",
    "    found_in_proposed_node_d.append(copy.deepcopy(proposed_node_tmp))\n",
    "    proposed_node_tmp.clear()\n",
    "\n",
    "for idx in range(len(found_in_basic_node_d)):\n",
    "    found_in_basic_node_d[idx] = list(set(found_in_basic_node_d[idx]))\n",
    "for idx in range(len(found_in_proposed_node_d)):\n",
    "    found_in_proposed_node_d[idx] = list(set(found_in_proposed_node_d[idx]))\n",
    "\n",
    "nodes_sum_basic = 0\n",
    "for item in found_in_basic_node_r:\n",
    "    nodes_sum_basic += len(item)\n",
    "print(f\"basic r 的 node 数量{nodes_sum_basic}\")\n",
    "nodes_sum_proposed = 0\n",
    "for item in found_in_proposed_node_r:\n",
    "    nodes_sum_proposed += len(item)\n",
    "print(f\"proposed r 的 node 数量{nodes_sum_proposed}\")\n",
    "results[str(unit_num * batch_num)].append((nodes_sum_basic, nodes_sum_proposed))\n",
    "time_consumed_max_basic, refuse_cnt_basic, accept_cnt_basic, serve_prob_basic = cal_time(found_in_basic_node_d,\n",
    "                                                                                         store_node_method_d_batched_basic_method)\n",
    "time_consumed_max_proposed, refuse_cnt_proposed, accept_cnt_proposed, serve_prob_proposed = cal_time(\n",
    "    found_in_proposed_node_d, store_node_method_d_batched_proposed_method)\n",
    "results[str(unit_num * batch_num)].append((time_consumed_max_basic, refuse_cnt_basic, accept_cnt_basic, serve_prob_basic))\n",
    "results[str(unit_num * batch_num)].append((time_consumed_max_proposed, refuse_cnt_proposed, accept_cnt_proposed, serve_prob_proposed))\n",
    "\n",
    "print(results)\n",
    "\n",
    "result_temp = {}\n",
    "for key, value in results.items():\n",
    "    if len(results[key]) != 0:\n",
    "        result_temp[key] = value\n",
    "# 将数据转换为DataFrame\n",
    "df = pd.DataFrame(result_temp)\n",
    "# 将DataFrame写入到Excel文件\n",
    "output_file = f'./output_batch_size_{unit_num * batch_num}.xlsx'\n",
    "df.to_excel(output_file, index=False)\n",
    "print(f'Data has been written to {output_file}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T15:04:46.230676Z",
     "start_time": "2024-07-01T15:04:39.114628Z"
    }
   },
   "id": "7ab2ccbf18a6dd39",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 10 20 30 [1] [2] [3]\n",
    "for item in range(0, 3):\n",
    "    unit_num, batch_num = unit_batch_num_list[item][0], unit_batch_num_list[item][1]\n",
    "    print(f\"unit_num: {unit_num} batch_num: {batch_num} batch size: {unit_num * batch_num}\")\n",
    "    batched_basic_method = {}  # 以字典存储所有的Batch batch_id : []\n",
    "    cnt = 0\n",
    "    for start_time in range(0, time_max, time_on_chain):\n",
    "        end_time = start_time + time_on_chain\n",
    "\n",
    "        data_points = []\n",
    "        for time in range(start_time, end_time):\n",
    "            for device in range(1, device_value_max + 1):\n",
    "                # data_points.append(data_aggregated.loc[time][device])\n",
    "                data_points.append((time, device))  # 400 个数据点\n",
    "        random.shuffle(data_points)  # 随机打乱\n",
    "\n",
    "        # 按照 unit_num 个 data_point 为一组打包\n",
    "        units_list = []\n",
    "        unit_tmp = []\n",
    "        # print(len(data_points))\n",
    "        for idx in range(0, len(data_points), unit_num):  # 以 5 为步长\n",
    "            for i in range(idx, idx + unit_num):\n",
    "                unit_tmp.append(data_points[i])\n",
    "            units_list.append(copy.deepcopy(unit_tmp))  # 得到 400 / 5 = 80 个 unit\n",
    "            unit_tmp.clear()\n",
    "        # print(len(units_list))\n",
    "        random.shuffle(units_list)\n",
    "\n",
    "        # 按照 batch_num 个 unit 为一组打包\n",
    "        batch_list = []\n",
    "        batch_tmp = []\n",
    "        for idx in range(0, len(units_list), batch_num):  # 得到 80 / 5 = 16 个 batch\n",
    "            for i in range(idx, idx + batch_num):\n",
    "                batch_tmp.append(units_list[i])\n",
    "            batch_list.append(copy.deepcopy(batch_tmp))\n",
    "            batch_tmp.clear()\n",
    "        for batch in batch_list:\n",
    "            batched_basic_method[str(cnt)] = batch\n",
    "            cnt += 1\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(device_value_max))\n",
    "\n",
    "    vDevice = 1\n",
    "    count = 0\n",
    "    for node in G.nodes():\n",
    "        # 为每个节点添加两个属性值\n",
    "        nx.set_node_attributes(G, {node: {'device_id': vDevice}})\n",
    "        count += 1\n",
    "        if vDevice <= device_value_max:\n",
    "            vDevice += 1\n",
    "\n",
    "    init_edge_weights(G)\n",
    "    batched_proposed_method = {}  # 以字典存储所有的Batch batch_id : []\n",
    "    cnt = 0\n",
    "\n",
    "    # 采样记录所有的Query\n",
    "    Query_sets = []\n",
    "    exec_time_list = []\n",
    "    for start_time in range(0, time_max, time_on_chain):\n",
    "        end_time = start_time + time_on_chain\n",
    "        units_list = []\n",
    "        unit_tmp = []\n",
    "        device_id = 1\n",
    "        while device_id <= device_value_max:\n",
    "            time_cnt = 0\n",
    "            for time in range(start_time, end_time):\n",
    "                # data_points.append(data_aggregated.loc[time][device])\n",
    "                unit_tmp.append((time, device_id))\n",
    "                time_cnt += 1\n",
    "                if time_cnt % unit_num == 0:\n",
    "                    units_list.append((device_id, copy.deepcopy(unit_tmp)))\n",
    "                    unit_tmp.clear()\n",
    "            device_id += 1\n",
    "\n",
    "        # 需要生成Query 并更新Device图 再进行分类，最后根据谱聚类结果 按照10个unit为一组打包为Batch\n",
    "        alpha = 0.8\n",
    "        beta = 0.5  # beta 比较适合 * 一个信息表示该此访问的 强度\n",
    "\n",
    "        num_queries = 10\n",
    "        # num_queries = int(time_on_chain * device_value_max / unit_num / batch_num / 2) # 16 / 2\n",
    "        # 本次生成的Query用于更新下一次的Device\n",
    "        query_set = generateQuery(num_queries, start_time, end_time - 1, device_value_max)\n",
    "        Query_sets.append(copy.deepcopy(query_set))  # 将每段on_chain时间内生成的query_set整合到循环外部一个总的Query集合中\n",
    "\n",
    "        # 更新Device, 从第二批数据开始，按照query更新\n",
    "\n",
    "        if start_time != 0:\n",
    "            # 记录开始时间\n",
    "            ratioCut_start_time = time_module.time()\n",
    "            for query in Query_sets[-1]:\n",
    "                queried_nodes = [node for node, attributes in G.nodes(data=True) if attributes['device_id'] >= query[1][0] and attributes['device_id'] <= query[1][1]]\n",
    "                for u, v, attrs in G.edges(data=True):\n",
    "                    if u in queried_nodes and v in queried_nodes:\n",
    "                        attrs['weight'] = alpha * G.edges[u, v]['weight'] + beta * G.edges[v, u]['weight']\n",
    "                    else:\n",
    "                        attrs['weight'] = alpha * G.edges[u, v]['weight']\n",
    "\n",
    "        print(f\"{unit_num * batch_num} : 迭代图完成\")\n",
    "        # 谱聚类\n",
    "        Adjacent = nx.adjacency_matrix(G)  # 获取邻接矩阵\n",
    "        total_sum = np.sum(Adjacent.data)  # 对稀疏矩阵中的每个非零元素进行标准化\n",
    "        normalized_adjacent = Adjacent / total_sum  # normalized_adjacent = normalized_adjacent * 1e6\n",
    "        # print(\"包含 NaN:\", np.isnan(normalized_adjacent).any())\n",
    "        Laplacian = calLaplacianMatrix(normalized_adjacent)\n",
    "        Laplacian = Laplacian.astype(np.float64)\n",
    "        # print(\"包含 NaN:\", np.isnan(Laplacian).any())\n",
    "        Laplacian = np.nan_to_num(Laplacian, nan=0.0)\n",
    "\n",
    "        x, V = np.linalg.eig(Laplacian)\n",
    "        x = zip(x, range(len(x)))\n",
    "        x = sorted(x, key=lambda x: x[0])\n",
    "        H = np.vstack([V[:, i] for (v, i) in x[:device_value_max]]).T\n",
    "        class_number = 10\n",
    "        # class_number = int(time_on_chain * device_value_max / unit_num / batch_num) # 类别数量 4\n",
    "        sp_kmeans = KMeans(n_clusters=class_number, n_init='auto').fit(H)\n",
    "        # sp_kmeans = KMeans(n_clusters=16, n_init='auto').fit(H)\n",
    "        labels = sp_kmeans.labels_  # labels 标记了每一个 device 所属的类别\n",
    "        nlist = list(G)  # 20 个 device\n",
    "        print(f\"{unit_num * batch_num} : 谱聚类完成\")\n",
    "        if start_time != 0:\n",
    "            # 记录结束时间\n",
    "            ratioCut_end_time = time_module.time()\n",
    "            # 计算运行时间\n",
    "            execution_time = ratioCut_end_time - ratioCut_start_time\n",
    "            execution_time_ms = execution_time * 1000\n",
    "            exec_time_list.append(execution_time_ms)\n",
    "\n",
    "        node2label = {}\n",
    "        for idx in range(len(nlist)):\n",
    "            node2label[str(nlist[idx] + 1)] = labels[idx]\n",
    "\n",
    "        # 统计列表中每个类别的数量\n",
    "        number_counts = Counter(labels)\n",
    "        class_counts = []\n",
    "\n",
    "        # 由于现在只要对unit 组合为 Batch ，而一个 unit 内的 data_point 都属于同一个 Device ，故只需要找到 20个 Device 与 类别 的映射关系\n",
    "        units_with_label = []\n",
    "        for (device_id, unit) in units_list:\n",
    "            label = node2label[str(device_id)]\n",
    "            units_with_label.append((label, copy.deepcopy(unit)))\n",
    "\n",
    "        # 将unit组合为Batch\n",
    "        batch_list = []\n",
    "        batch_tmp = []\n",
    "        # 20个 Device 被归为了4类，现在遍历所有的unit , 根据unit中的data point的 device_id 决定其所属的类别 class\n",
    "        batch_cnt = 0\n",
    "        units_with_label.sort(key=lambda x: x[0])  # 按照labels对units进行升序排序\n",
    "        for (label, unit) in units_with_label:\n",
    "            batch_tmp.append(copy.deepcopy(unit))\n",
    "            batch_cnt += 1\n",
    "            if batch_cnt % batch_num == 0:\n",
    "                batch_list.append(copy.deepcopy(batch_tmp))\n",
    "                batch_tmp.clear()\n",
    "\n",
    "        for batch in batch_list:\n",
    "            batched_proposed_method[str(cnt)] = batch\n",
    "            cnt += 1\n",
    "        print(f\"{unit_num * batch_num} : batched_proposed_method完成\")\n",
    "    # 计算出RatioCut所需的平均时间\n",
    "    sum = 0.0\n",
    "    for item in exec_time_list:\n",
    "        sum += item\n",
    "    exec_time_avg = sum / len(exec_time_list)\n",
    "    print(exec_time_avg)\n",
    "    results[str(unit_num * batch_num)].append(exec_time_avg)\n",
    "\n",
    "    # 方法一:\n",
    "\n",
    "    store_node_method_r_d = copy.deepcopy(store_node)\n",
    "    # batched_basic_method\n",
    "    store_node_method_r_d_batched_basic_method = copy.deepcopy(store_node_method_r_d)\n",
    "\n",
    "    for key, batch in batched_basic_method.items():\n",
    "        # 要选取一个store node存储\n",
    "        sorted_nodes_r_d = sorted(store_node_method_r_d_batched_basic_method.items(),\n",
    "                                  key=lambda x: x[1]['score_sd'], reverse=False)\n",
    "        # 从得分前 10 的节点中随机选择一个节点\n",
    "        top_ten_nodes_r_d = sorted_nodes_r_d[:10]\n",
    "\n",
    "        saved_flag = False\n",
    "        for node in top_ten_nodes_r_d:\n",
    "            if saved_flag:\n",
    "                break\n",
    "            get_pro = node[1]['probability']\n",
    "            # 得到命中概率\n",
    "            if probabilistic_true(get_pro):\n",
    "                # 若命中，则存入\n",
    "                store_node_method_r_d_batched_basic_method[node[0]]['batches'].append((key, batch))\n",
    "                saved_flag = True\n",
    "        if not saved_flag:\n",
    "            # 10 个都没命中，则存入 rank 第一的 node 中\n",
    "            store_node_method_r_d_batched_basic_method[top_ten_nodes_r_d[0][0]]['batches'].append((key, batch))\n",
    "\n",
    "    # batched_proposed_method\n",
    "    store_node_method_r_d_batched_proposed_method = copy.deepcopy(store_node_method_r_d)\n",
    "\n",
    "    for key, batch in batched_proposed_method.items():\n",
    "        # 要选取一个store node存储\n",
    "        sorted_nodes = sorted(store_node_method_r_d_batched_proposed_method.items(), key=lambda x: x[1]['score_sd'],\n",
    "                              reverse=False)  # 考虑 Distance 升序，距离小的优先\n",
    "        # 从得分前 10 的节点中随机选择一个节点\n",
    "        top_ten_nodes = sorted_nodes[:10]\n",
    "        saved_flag = False\n",
    "        for node in top_ten_nodes:\n",
    "            if saved_flag:\n",
    "                break\n",
    "            get_pro = node[1]['probability']\n",
    "            # 得到命中概率\n",
    "            if probabilistic_true(get_pro):\n",
    "                # 若命中，则存入\n",
    "                store_node_method_r_d_batched_proposed_method[node[0]]['batches'].append(\n",
    "                    (key, batch))  # 考虑 reputation\n",
    "                saved_flag = True\n",
    "        if not saved_flag:\n",
    "            # 10 个都没命中，则存入 rank 第一的 node 中\n",
    "            store_node_method_r_d_batched_proposed_method[top_ten_nodes[0][0]]['batches'].append((key, batch))\n",
    "\n",
    "    # 遍历Query_sets，在两种store node的存储方式上，访问所有的query，计算得到指标\n",
    "    found_in_basic_batch = []  # [[], [], ..., []] 记录了每个 query 中每个 point 所在的batch_id ， 用于后续去 store\n",
    "    found_in_proposed_batch = []\n",
    "    for query_set in Query_sets:\n",
    "        basic_batches4q = []\n",
    "        proposed_batches4q = []\n",
    "        for query in query_set:\n",
    "            # 对每一个 q, 得到 time 和 device\n",
    "            time_low, time_high, device_low, device_high = query[0][0], query[0][1], query[1][0], query[1][1]\n",
    "            # print(f\"time_low:{time_low}, time_high:{time_high}, device_low:{device_low}, device_high:{device_high}\")\n",
    "            # 找到 device 所在的 batch, 计算出每个 q 需要访问哪些 batch\n",
    "            # 直接遍历搜索空间过大，进行优化: 从time来确定Batch_id范围: 0~19s 内的数据处于 0~3 的 batch 之间\n",
    "            # 由于 同一批Query_set中的元素都在同一批Batch中，故直接由time_low定出 batch range\n",
    "            batch_low, batch_high = int(time_low / time_on_chain) * int(\n",
    "                time_on_chain * device_value_max / unit_num / batch_num), (int(time_low / time_on_chain) + 1) * int(\n",
    "                time_on_chain * device_value_max / unit_num / batch_num) - 1\n",
    "            # print(f\"batch_low:{batch_low} and batch_high:{batch_high}\")\n",
    "            for device in range(device_low, device_high + 1):\n",
    "                for time in range(time_low, time_high + 1):\n",
    "                    # print(f\"time: {time}, device: {device}\")\n",
    "                    basic_flag = False\n",
    "                    proposed_flag = False\n",
    "                    # 对每个 （time, device） 找到所属的Batch\n",
    "                    for batch_id in range(batch_low, batch_high + 1):\n",
    "                        # print(f\"batch id is : {batch}\")\n",
    "                        # batch结构: [[], [], ..., []]\n",
    "\n",
    "                        if not basic_flag:\n",
    "                            batch = copy.deepcopy(batched_basic_method[str(batch_id)])\n",
    "                            for unit in batch:\n",
    "                                for point in unit:\n",
    "                                    # print(f\"{point[0]}  and  {point[1]}\")\n",
    "                                    if time == point[0] and device == point[1]:\n",
    "                                        # print(f\"----basic find point\")\n",
    "                                        # 找到了所属的Batch，继续寻找所属的store node\n",
    "                                        basic_batches4q.append(batch_id)\n",
    "                                        basic_flag = True\n",
    "\n",
    "                        if not proposed_flag:\n",
    "                            batch = copy.deepcopy(batched_proposed_method[str(batch_id)])\n",
    "                            for unit in batch:\n",
    "                                for point in unit:\n",
    "                                    if time == point[0] and device == point[1]:\n",
    "                                        # print(f\"----proposed find point\")\n",
    "                                        # 找到了所属的Batch，继续寻找所属的store node\n",
    "                                        proposed_batches4q.append(batch_id)\n",
    "                                        proposed_flag = True\n",
    "                        if proposed_flag and basic_flag:\n",
    "                            break\n",
    "                    # print(f\"belong_to_basic_batch:{belong_to_basic_batch} and belong_to_proposed_batch:{belong_to_proposed_batch}\")\n",
    "            found_in_basic_batch.append(copy.deepcopy(basic_batches4q))\n",
    "            found_in_proposed_batch.append(copy.deepcopy(proposed_batches4q))\n",
    "            basic_batches4q.clear()\n",
    "            proposed_batches4q.clear()\n",
    "\n",
    "    for idx in range(len(found_in_basic_batch)):\n",
    "        found_in_basic_batch[idx] = list(set(found_in_basic_batch[idx]))\n",
    "\n",
    "    for idx in range(len(found_in_proposed_batch)):\n",
    "        found_in_proposed_batch[idx] = list(set(found_in_proposed_batch[idx]))\n",
    "\n",
    "    batches_sum_basic = 0\n",
    "    for item in found_in_basic_batch:\n",
    "        batches_sum_basic += len(item)\n",
    "    print(f\"basic 的 batch 数量{batches_sum_basic}\")\n",
    "\n",
    "    batches_sum_proposed = 0\n",
    "    for item in found_in_proposed_batch:\n",
    "        batches_sum_proposed += len(item)\n",
    "    print(f\"proposed 的 batch 数量{batches_sum_proposed}\")\n",
    "\n",
    "    results[str(unit_num * batch_num)].append((batches_sum_basic, batches_sum_proposed))\n",
    "\n",
    "    # found_in_basic_node\n",
    "    found_in_basic_node_r_d = []\n",
    "    basic_node_tmp = []\n",
    "    for query in found_in_basic_batch:\n",
    "        # print(query)\n",
    "        found_batches = list(set(query))\n",
    "        # print(found_batches)\n",
    "        for batch_id in found_batches:\n",
    "            # 首先要合并所有相同的 batch_id , 再对每个不同的Batch_id到store_node中找到 node_id 存入found_in_xxx_node\n",
    "            flag = False\n",
    "            for node_id, node_info in store_node_method_r_d_batched_basic_method.items():\n",
    "                if flag:\n",
    "                    break\n",
    "                for key, value in node_info['batches']:\n",
    "                    if int(batch_id) == int(key):\n",
    "                        basic_node_tmp.append(node_id)\n",
    "                        flag = True\n",
    "                        break\n",
    "        found_in_basic_node_r_d.append(copy.deepcopy(basic_node_tmp))\n",
    "        basic_node_tmp.clear()\n",
    "\n",
    "    # found_in_proposed_node\n",
    "    found_in_proposed_node_r_d = []\n",
    "    proposed_node_tmp = []\n",
    "    for query in found_in_proposed_batch:\n",
    "        # print(query)\n",
    "        found_batches = list(set(query))\n",
    "        # print(found_batches)\n",
    "        for batch_id in found_batches:\n",
    "            # 首先要合并所有相同的 batch_id , 再对每个不同的Batch_id到store_node中找到 node_id 存入found_in_xxx_node\n",
    "            flag = False\n",
    "            for node_id, node_info in store_node_method_r_d_batched_proposed_method.items():\n",
    "                if flag:\n",
    "                    break\n",
    "                for key, value in node_info['batches']:\n",
    "                    if int(batch_id) == int(key):\n",
    "                        proposed_node_tmp.append(node_id)\n",
    "                        flag = True\n",
    "                        break\n",
    "        found_in_proposed_node_r_d.append(copy.deepcopy(proposed_node_tmp))\n",
    "        proposed_node_tmp.clear()\n",
    "\n",
    "    for idx in range(len(found_in_basic_node_r_d)):\n",
    "        found_in_basic_node_r_d[idx] = list(set(found_in_basic_node_r_d[idx]))\n",
    "    for idx in range(len(found_in_proposed_node_r_d)):\n",
    "        found_in_proposed_node_r_d[idx] = list(set(found_in_proposed_node_r_d[idx]))\n",
    "\n",
    "    nodes_sum_basic = 0\n",
    "    for item in found_in_basic_node_r_d:\n",
    "        nodes_sum_basic += len(item)\n",
    "    print(f\"basic r_d 的 node 数量{nodes_sum_basic}\")\n",
    "    nodes_sum_proposed = 0\n",
    "    for item in found_in_proposed_node_r_d:\n",
    "        nodes_sum_proposed += len(item)\n",
    "    print(f\"proposed r_d 的 node 数量{nodes_sum_proposed}\")\n",
    "    results[str(unit_num * batch_num)].append((nodes_sum_basic, nodes_sum_proposed))\n",
    "\n",
    "    time_consumed_max_basic, refuse_cnt_basic, accept_cnt_basic, serve_prob_basic = cal_time(\n",
    "        found_in_basic_node_r_d, store_node_method_r_d_batched_basic_method)\n",
    "    time_consumed_max_proposed, refuse_cnt_proposed, accept_cnt_proposed, serve_prob_proposed = cal_time(\n",
    "        found_in_proposed_node_r_d, store_node_method_r_d_batched_proposed_method)\n",
    "    results[str(unit_num * batch_num)].append((time_consumed_max_basic, refuse_cnt_basic, accept_cnt_basic, serve_prob_basic))\n",
    "    results[str(unit_num * batch_num)].append((time_consumed_max_proposed, refuse_cnt_proposed, accept_cnt_proposed, serve_prob_proposed))\n",
    "\n",
    "    # 方法二:\n",
    "    store_node_method_r = copy.deepcopy(store_node)\n",
    "    store_node_method_r_batched_basic_method = copy.deepcopy(store_node_method_r)\n",
    "\n",
    "    for key, batch in batched_basic_method.items():\n",
    "        # 要选取一个store node存储\n",
    "        sorted_nodes_r = sorted(store_node_method_r_batched_basic_method.items(), key=lambda x: x[1]['score_sp'],\n",
    "                                reverse=True)  # Ture 指定为降序排序\n",
    "\n",
    "        # 从得分前 10 的节点中随机选择一个节点\n",
    "        top_ten_nodes_r = sorted_nodes_r[:10]\n",
    "        saved_flag = False\n",
    "        for node in top_ten_nodes_r:\n",
    "            if saved_flag:\n",
    "                break\n",
    "            get_pro = node[1]['probability']\n",
    "            # 得到命中概率\n",
    "            if probabilistic_true(get_pro):\n",
    "                # 若命中，则存入\n",
    "                store_node_method_r_batched_basic_method[node[0]]['batches'].append((key, batch))\n",
    "                saved_flag = True\n",
    "\n",
    "        if not saved_flag:\n",
    "            # 10 个都没命中，则存入 rank 第一的 node 中\n",
    "            store_node_method_r_batched_basic_method[top_ten_nodes_r[0][0]]['batches'].append((key, batch))\n",
    "\n",
    "    store_node_method_r_batched_proposed_method = copy.deepcopy(store_node_method_r)\n",
    "\n",
    "    for key, batch in batched_proposed_method.items():\n",
    "        # 要选取一个store node存储\n",
    "        sorted_nodes_r = sorted(store_node_method_r_batched_proposed_method.items(), key=lambda x: x[1]['score_sp'],\n",
    "                                reverse=True)\n",
    "        # 从得分前 10 的节点中随机选择一个节点\n",
    "        top_ten_nodes_r = sorted_nodes_r[:10]\n",
    "        saved_flag = False\n",
    "        for node in top_ten_nodes_r:\n",
    "            if saved_flag:\n",
    "                break\n",
    "            get_pro = node[1]['probability']\n",
    "            # 得到命中概率\n",
    "            if probabilistic_true(get_pro):\n",
    "                # 若命中，则存入\n",
    "                store_node_method_r_batched_proposed_method[node[0]]['batches'].append((key, batch))\n",
    "                saved_flag = True\n",
    "        if not saved_flag:\n",
    "            # 10 个都没命中，则存入 rank 第一的 node 中\n",
    "            store_node_method_r_batched_proposed_method[top_ten_nodes_r[0][0]]['batches'].append((key, batch))\n",
    "\n",
    "    # 遍历Query_sets，在两种store node的存储方式上，访问所有的query，计算得到指标\n",
    "    found_in_basic_batch_r = []  # [[], [], ..., []] 记录了每个 query 中每个 point 所在的batch_id ， 用于后续去 store\n",
    "    found_in_proposed_batch_r = []\n",
    "    for query_set in Query_sets:\n",
    "        basic_batches4q = []\n",
    "        proposed_batches4q = []\n",
    "        for query in query_set:\n",
    "            # 对每一个 q, 得到 time 和 device\n",
    "            time_low, time_high, device_low, device_high = query[0][0], query[0][1], query[1][0], query[1][1]\n",
    "            # print(f\"time_low:{time_low}, time_high:{time_high}, device_low:{device_low}, device_high:{device_high}\")\n",
    "            # 找到 device 所在的 batch, 计算出每个 q 需要访问哪些 batch\n",
    "            # 直接遍历搜索空间过大，进行优化: 从time来确定Batch_id范围: 0~19s 内的数据处于 0~3 的 batch 之间\n",
    "            # 由于 同一批Query_set中的元素都在同一批Batch中，故直接由time_low定出 batch range\n",
    "            batch_low, batch_high = int(time_low / time_on_chain) * int(\n",
    "                time_on_chain * device_value_max / unit_num / batch_num), (int(time_low / time_on_chain) + 1) * int(\n",
    "                time_on_chain * device_value_max / unit_num / batch_num) - 1\n",
    "            # print(f\"batch_low:{batch_low} and batch_high:{batch_high}\")\n",
    "            for device in range(device_low, device_high + 1):\n",
    "                for time in range(time_low, time_high + 1):\n",
    "                    # print(f\"time: {time}, device: {device}\")\n",
    "                    basic_flag = False\n",
    "                    proposed_flag = False\n",
    "                    # 对每个 （time, device） 找到所属的Batch\n",
    "                    for batch_id in range(batch_low, batch_high + 1):\n",
    "                        # print(f\"batch id is : {batch}\")\n",
    "                        # batch结构: [[], [], ..., []]\n",
    "                        if not basic_flag:\n",
    "                            batch = copy.deepcopy(batched_basic_method[str(batch_id)])\n",
    "                            for unit in batch:\n",
    "                                for point in unit:\n",
    "                                    # print(f\"{point[0]}  and  {point[1]}\")\n",
    "                                    if time == point[0] and device == point[1]:\n",
    "                                        # print(f\"----basic find point\")\n",
    "                                        # 找到了所属的Batch，继续寻找所属的store node\n",
    "                                        basic_batches4q.append(batch_id)\n",
    "                                        basic_flag = True\n",
    "                        if not proposed_flag:\n",
    "                            batch = copy.deepcopy(batched_proposed_method[str(batch_id)])\n",
    "                            for unit in batch:\n",
    "                                for point in unit:\n",
    "                                    if time == point[0] and device == point[1]:\n",
    "                                        # print(f\"----proposed find point\")\n",
    "                                        # 找到了所属的Batch，继续寻找所属的store node\n",
    "                                        proposed_batches4q.append(batch_id)\n",
    "                                        proposed_flag = True\n",
    "                        if proposed_flag and basic_flag:\n",
    "                            break\n",
    "                    # print(f\"belong_to_basic_batch:{belong_to_basic_batch} and belong_to_proposed_batch:{belong_to_proposed_batch}\")\n",
    "            found_in_basic_batch_r.append(copy.deepcopy(basic_batches4q))\n",
    "            found_in_proposed_batch_r.append(copy.deepcopy(proposed_batches4q))\n",
    "            basic_batches4q.clear()\n",
    "            proposed_batches4q.clear()\n",
    "\n",
    "    for idx in range(len(found_in_basic_batch_r)):\n",
    "        found_in_basic_batch_r[idx] = list(set(found_in_basic_batch_r[idx]))\n",
    "\n",
    "    for idx in range(len(found_in_proposed_batch_r)):\n",
    "        found_in_proposed_batch_r[idx] = list(set(found_in_proposed_batch_r[idx]))\n",
    "\n",
    "    found_in_basic_node_r = []\n",
    "    basic_node_tmp = []\n",
    "    for query in found_in_basic_batch_r:\n",
    "        # print(query)\n",
    "        found_batches = list(set(query))\n",
    "        # print(found_batches)\n",
    "        for batch_id in found_batches:\n",
    "            # 首先要合并所有相同的 batch_id , 再对每个不同的Batch_id到store_node中找到 node_id 存入found_in_xxx_node\n",
    "            flag = False\n",
    "            for node_id, node_info in store_node_method_r_batched_basic_method.items():\n",
    "                if flag:\n",
    "                    break\n",
    "                for key, value in node_info['batches']:\n",
    "                    if int(batch_id) == int(key):\n",
    "                        basic_node_tmp.append(node_id)\n",
    "                        flag = True\n",
    "                        break\n",
    "        found_in_basic_node_r.append(copy.deepcopy(basic_node_tmp))\n",
    "        basic_node_tmp.clear()\n",
    "\n",
    "    # found_in_proposed_node\n",
    "    found_in_proposed_node_r = []\n",
    "    proposed_node_tmp = []\n",
    "    for query in found_in_proposed_batch:\n",
    "        # print(query)\n",
    "        found_batches = list(set(query))\n",
    "        # print(found_batches)\n",
    "        for batch_id in found_batches:\n",
    "            # 首先要合并所有相同的 batch_id , 再对每个不同的Batch_id到store_node中找到 node_id 存入found_in_xxx_node\n",
    "            flag = False\n",
    "            for node_id, node_info in store_node_method_r_batched_proposed_method.items():\n",
    "                if flag:\n",
    "                    break\n",
    "                for key, value in node_info['batches']:\n",
    "                    if int(batch_id) == int(key):\n",
    "                        proposed_node_tmp.append(node_id)\n",
    "                        flag = True\n",
    "                        break\n",
    "        found_in_proposed_node_r.append(copy.deepcopy(proposed_node_tmp))\n",
    "        proposed_node_tmp.clear()\n",
    "\n",
    "    for idx in range(len(found_in_basic_node_r)):\n",
    "        found_in_basic_node_r[idx] = list(set(found_in_basic_node_r[idx]))\n",
    "\n",
    "    for idx in range(len(found_in_proposed_node_r)):\n",
    "        found_in_proposed_node_r[idx] = list(set(found_in_proposed_node_r[idx]))\n",
    "\n",
    "    nodes_sum_basic = 0\n",
    "    for item in found_in_basic_node_r:\n",
    "        nodes_sum_basic += len(item)\n",
    "    print(f\"basic r 的 node 数量{nodes_sum_basic}\")\n",
    "    nodes_sum_proposed = 0\n",
    "    for item in found_in_proposed_node_r:\n",
    "        nodes_sum_proposed += len(item)\n",
    "    print(f\"proposed r 的 node 数量{nodes_sum_proposed}\")\n",
    "\n",
    "    results[str(unit_num * batch_num)].append((nodes_sum_basic, nodes_sum_proposed))\n",
    "\n",
    "    time_consumed_max_basic, refuse_cnt_basic, accept_cnt_basic, serve_prob_basic = cal_time(found_in_basic_node_r, store_node_method_r_batched_basic_method)\n",
    "    time_consumed_max_proposed, refuse_cnt_proposed, accept_cnt_proposed, serve_prob_proposed = cal_time(found_in_proposed_node_r, store_node_method_r_batched_proposed_method)\n",
    "\n",
    "    results[str(unit_num * batch_num)].append((time_consumed_max_basic, refuse_cnt_basic, accept_cnt_basic, serve_prob_basic))\n",
    "    results[str(unit_num * batch_num)].append((time_consumed_max_proposed, refuse_cnt_proposed, accept_cnt_proposed, serve_prob_proposed))\n",
    "\n",
    "\n",
    "    # 方法三\n",
    "    store_node_method_d = copy.deepcopy(store_node)\n",
    "\n",
    "    # batched_basic_method\n",
    "    store_node_method_d_batched_basic_method = copy.deepcopy(store_node_method_d)\n",
    "\n",
    "    for key, batch in batched_basic_method.items():\n",
    "        # 要选取一个store node存储\n",
    "        sorted_nodes_d = sorted(store_node_method_d_batched_basic_method.items(), key=lambda x: x[1]['score_sd'],\n",
    "                                reverse=False)  # 升序排序，距离越小越优先\n",
    "        # 从得分前 10 的节点中随机选择一个节点\n",
    "        top_ten_nodes = sorted_nodes_d[:10]\n",
    "        random_choice_node = random.choice(top_ten_nodes)\n",
    "        store_node_method_d_batched_basic_method[random_choice_node[0]]['batches'].append((key, batch))\n",
    "\n",
    "    store_node_method_d_batched_proposed_method = copy.deepcopy(store_node_method_d)\n",
    "\n",
    "    for key, batch in batched_proposed_method.items():\n",
    "        # 要选取一个store node存储\n",
    "        sorted_nodes_d = sorted(store_node_method_d_batched_proposed_method.items(), key=lambda x: x[1]['score_sd'],\n",
    "                                reverse=False)  # 升序排序，距离越小越优先\n",
    "        # 从得分前 10 的节点中随机选择一个节点\n",
    "        top_ten_nodes = sorted_nodes_d[:10]\n",
    "        random_choice_node = random.choice(top_ten_nodes)\n",
    "        store_node_method_d_batched_proposed_method[random_choice_node[0]]['batches'].append((key, batch))\n",
    "\n",
    "    # 遍历Query_sets，在两种store node的存储方式上，访问所有的query，计算得到指标\n",
    "    found_in_basic_batch = []  # [[], [], ..., []] 记录了每个 query 中每个 point 所在的batch_id ， 用于后续去 store\n",
    "    found_in_proposed_batch = []\n",
    "    found_in_basic_batch.clear()\n",
    "    found_in_proposed_batch.clear()\n",
    "    for query_set in Query_sets:\n",
    "        basic_batches4q = []\n",
    "        proposed_batches4q = []\n",
    "        for query in query_set:\n",
    "            # 对每一个 q, 得到 time 和 device\n",
    "            time_low, time_high, device_low, device_high = query[0][0], query[0][1], query[1][0], query[1][1]\n",
    "            # print(f\"time_low:{time_low}, time_high:{time_high}, device_low:{device_low}, device_high:{device_high}\")\n",
    "            # 找到 device 所在的 batch, 计算出每个 q 需要访问哪些 batch\n",
    "            # 直接遍历搜索空间过大，进行优化: 从time来确定Batch_id范围: 0~19s 内的数据处于 0~3 的 batch 之间\n",
    "            # 由于 同一批Query_set中的元素都在同一批Batch中，故直接由time_low定出 batch range\n",
    "            batch_low, batch_high = int(time_low / time_on_chain) * int(\n",
    "                time_on_chain * device_value_max / unit_num / batch_num), (int(time_low / time_on_chain) + 1) * int(\n",
    "                time_on_chain * device_value_max / unit_num / batch_num) - 1\n",
    "            # print(f\"batch_low:{batch_low} and batch_high:{batch_high}\")\n",
    "            for device in range(device_low, device_high + 1):\n",
    "                for time in range(time_low, time_high + 1):\n",
    "                    # print(f\"time: {time}, device: {device}\")\n",
    "                    basic_flag = False\n",
    "                    proposed_flag = False\n",
    "                    # 对每个 （time, device） 找到所属的Batch\n",
    "                    for batch_id in range(batch_low, batch_high + 1):\n",
    "                        # print(f\"batch id is : {batch}\")\n",
    "                        # batch结构: [[], [], ..., []]\n",
    "\n",
    "                        if not basic_flag:\n",
    "                            batch = copy.deepcopy(batched_basic_method[str(batch_id)])\n",
    "                            for unit in batch:\n",
    "                                for point in unit:\n",
    "                                    # print(f\"{point[0]}  and  {point[1]}\")\n",
    "                                    if time == point[0] and device == point[1]:\n",
    "                                        # print(f\"----basic find point\")\n",
    "                                        # 找到了所属的Batch，继续寻找所属的store node\n",
    "                                        basic_batches4q.append(batch_id)\n",
    "                                        basic_flag = True\n",
    "\n",
    "                        if not proposed_flag:\n",
    "                            batch = copy.deepcopy(batched_proposed_method[str(batch_id)])\n",
    "                            for unit in batch:\n",
    "                                for point in unit:\n",
    "                                    if time == point[0] and device == point[1]:\n",
    "                                        # print(f\"----proposed find point\")\n",
    "                                        # 找到了所属的Batch，继续寻找所属的store node\n",
    "                                        proposed_batches4q.append(batch_id)\n",
    "                                        proposed_flag = True\n",
    "                        if proposed_flag and basic_flag:\n",
    "                            break\n",
    "                    # print(f\"belong_to_basic_batch:{belong_to_basic_batch} and belong_to_proposed_batch:{belong_to_proposed_batch}\")\n",
    "            found_in_basic_batch.append(copy.deepcopy(basic_batches4q))\n",
    "            found_in_proposed_batch.append(copy.deepcopy(proposed_batches4q))\n",
    "            basic_batches4q.clear()\n",
    "            proposed_batches4q.clear()\n",
    "\n",
    "    for idx in range(len(found_in_basic_batch)):\n",
    "        found_in_basic_batch[idx] = list(set(found_in_basic_batch[idx]))\n",
    "    for idx in range(len(found_in_proposed_batch)):\n",
    "        found_in_proposed_batch[idx] = list(set(found_in_proposed_batch[idx]))\n",
    "\n",
    "    found_in_basic_node_d = []\n",
    "    basic_node_tmp = []\n",
    "    for query in found_in_basic_batch:\n",
    "        # print(query)\n",
    "        found_batches = list(set(query))\n",
    "        # print(found_batches)\n",
    "        for batch_id in found_batches:\n",
    "            # 首先要合并所有相同的 batch_id , 再对每个不同的Batch_id到store_node中找到 node_id 存入found_in_xxx_node\n",
    "            flag = False\n",
    "            for node_id, node_info in store_node_method_d_batched_basic_method.items():\n",
    "                if flag:\n",
    "                    break\n",
    "                for key, value in node_info['batches']:\n",
    "                    if int(batch_id) == int(key):\n",
    "                        basic_node_tmp.append(node_id)\n",
    "                        flag = True\n",
    "                        break\n",
    "        found_in_basic_node_d.append(copy.deepcopy(basic_node_tmp))\n",
    "        basic_node_tmp.clear()\n",
    "\n",
    "    # found_in_proposed_node\n",
    "    found_in_proposed_node_d = []\n",
    "    proposed_node_tmp = []\n",
    "    for query in found_in_proposed_batch:\n",
    "        # print(query)\n",
    "        found_batches = list(set(query))\n",
    "        # print(found_batches)\n",
    "        for batch_id in found_batches:\n",
    "            # 首先要合并所有相同的 batch_id , 再对每个不同的Batch_id到store_node中找到 node_id 存入found_in_xxx_node\n",
    "            flag = False\n",
    "            for node_id, node_info in store_node_method_d_batched_proposed_method.items():\n",
    "                if flag:\n",
    "                    break\n",
    "                for key, value in node_info['batches']:\n",
    "                    if int(batch_id) == int(key):\n",
    "                        proposed_node_tmp.append(node_id)\n",
    "                        flag = True\n",
    "                        break\n",
    "        found_in_proposed_node_d.append(copy.deepcopy(proposed_node_tmp))\n",
    "        proposed_node_tmp.clear()\n",
    "\n",
    "    for idx in range(len(found_in_basic_node_d)):\n",
    "        found_in_basic_node_d[idx] = list(set(found_in_basic_node_d[idx]))\n",
    "    for idx in range(len(found_in_proposed_node_d)):\n",
    "        found_in_proposed_node_d[idx] = list(set(found_in_proposed_node_d[idx]))\n",
    "\n",
    "    nodes_sum_basic = 0\n",
    "    for item in found_in_basic_node_d:\n",
    "        nodes_sum_basic += len(item)\n",
    "    print(f\"basic r 的 node 数量{nodes_sum_basic}\")\n",
    "    nodes_sum_proposed = 0\n",
    "    for item in found_in_proposed_node_d:\n",
    "        nodes_sum_proposed += len(item)\n",
    "    print(f\"proposed r 的 node 数量{nodes_sum_proposed}\")\n",
    "\n",
    "    results[str(unit_num * batch_num)].append((nodes_sum_basic, nodes_sum_proposed))\n",
    "\n",
    "    time_consumed_max_basic, refuse_cnt_basic, accept_cnt_basic, serve_prob_basic = (\n",
    "        cal_time(found_in_basic_node_d, store_node_method_d_batched_basic_method))\n",
    "    time_consumed_max_proposed, refuse_cnt_proposed, accept_cnt_proposed, serve_prob_proposed =\\\n",
    "        cal_time(found_in_proposed_node_d, store_node_method_d_batched_proposed_method)\n",
    "\n",
    "    results[str(unit_num * batch_num)].append(\n",
    "        (time_consumed_max_basic, refuse_cnt_basic, accept_cnt_basic, serve_prob_basic))\n",
    "    results[str(unit_num * batch_num)].append(\n",
    "        (time_consumed_max_proposed, refuse_cnt_proposed, accept_cnt_proposed, serve_prob_proposed))\n",
    "\n",
    "    result_temp = {}\n",
    "    for key, value in results.items():\n",
    "        if len(results[key]) != 0:\n",
    "            result_temp[key] = value\n",
    "    df = pd.DataFrame(result_temp)\n",
    "    # 将DataFrame写入到Excel文件\n",
    "    output_file = f'./output_batch_size_{unit_num * batch_num}.xlsx'\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(f'Data has been written to {output_file}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T21:23:35.106480Z",
     "start_time": "2024-07-01T15:08:27.420568Z"
    }
   },
   "id": "82d76680bfda3943",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 10 20 30 [1] [2] [3]\n",
    "for item in range(6, 7):\n",
    "    unit_num, batch_num = unit_batch_num_list[item][0], unit_batch_num_list[item][1]\n",
    "    print(f\"unit_num: {unit_num} batch_num: {batch_num} batch size: {unit_num * batch_num}\")\n",
    "    batched_basic_method = {}  # 以字典存储所有的Batch batch_id : []\n",
    "    cnt = 0\n",
    "    for start_time in range(0, time_max, time_on_chain):\n",
    "        end_time = start_time + time_on_chain\n",
    "\n",
    "        data_points = []\n",
    "        for time in range(start_time, end_time):\n",
    "            for device in range(1, device_value_max + 1):\n",
    "                # data_points.append(data_aggregated.loc[time][device])\n",
    "                data_points.append((time, device))  # 400 个数据点\n",
    "        random.shuffle(data_points)  # 随机打乱\n",
    "\n",
    "        # 按照 unit_num 个 data_point 为一组打包\n",
    "        units_list = []\n",
    "        unit_tmp = []\n",
    "        # print(len(data_points))\n",
    "        for idx in range(0, len(data_points), unit_num):  # 以 5 为步长\n",
    "            for i in range(idx, idx + unit_num):\n",
    "                unit_tmp.append(data_points[i])\n",
    "            units_list.append(copy.deepcopy(unit_tmp))  # 得到 400 / 5 = 80 个 unit\n",
    "            unit_tmp.clear()\n",
    "        # print(len(units_list))\n",
    "        random.shuffle(units_list)\n",
    "\n",
    "        # 按照 batch_num 个 unit 为一组打包\n",
    "        batch_list = []\n",
    "        batch_tmp = []\n",
    "        for idx in range(0, len(units_list), batch_num):  # 得到 80 / 5 = 16 个 batch\n",
    "            for i in range(idx, idx + batch_num):\n",
    "                batch_tmp.append(units_list[i])\n",
    "            batch_list.append(copy.deepcopy(batch_tmp))\n",
    "            batch_tmp.clear()\n",
    "        for batch in batch_list:\n",
    "            batched_basic_method[str(cnt)] = batch\n",
    "            cnt += 1\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(device_value_max))\n",
    "\n",
    "    vDevice = 1\n",
    "    count = 0\n",
    "    for node in G.nodes():\n",
    "        # 为每个节点添加两个属性值\n",
    "        nx.set_node_attributes(G, {node: {'device_id': vDevice}})\n",
    "        count += 1\n",
    "        if vDevice <= device_value_max:\n",
    "            vDevice += 1\n",
    "\n",
    "    init_edge_weights(G)\n",
    "    batched_proposed_method = {}  # 以字典存储所有的Batch batch_id : []\n",
    "    cnt = 0\n",
    "\n",
    "    # 采样记录所有的Query\n",
    "    Query_sets = []\n",
    "    exec_time_list = []\n",
    "    for start_time in range(0, time_max, time_on_chain):\n",
    "        end_time = start_time + time_on_chain\n",
    "        units_list = []\n",
    "        unit_tmp = []\n",
    "        device_id = 1\n",
    "        while device_id <= device_value_max:\n",
    "            time_cnt = 0\n",
    "            for time in range(start_time, end_time):\n",
    "                # data_points.append(data_aggregated.loc[time][device])\n",
    "                unit_tmp.append((time, device_id))\n",
    "                time_cnt += 1\n",
    "                if time_cnt % unit_num == 0:\n",
    "                    units_list.append((device_id, copy.deepcopy(unit_tmp)))\n",
    "                    unit_tmp.clear()\n",
    "            device_id += 1\n",
    "\n",
    "        # 需要生成Query 并更新Device图 再进行分类，最后根据谱聚类结果 按照10个unit为一组打包为Batch\n",
    "        alpha = 0.8\n",
    "        beta = 0.5  # beta 比较适合 * 一个信息表示该此访问的 强度\n",
    "        print(f\"{unit_num * batch_num} : start 生成query\")\n",
    "        num_queries = 10\n",
    "        # num_queries = int(time_on_chain * device_value_max / unit_num / batch_num / 2) # 16 / 2\n",
    "        # 本次生成的Query用于更新下一次的Device\n",
    "        query_set = generateQuery(num_queries, start_time, end_time - 1, device_value_max)\n",
    "        Query_sets.append(copy.deepcopy(query_set))  # 将每段on_chain时间内生成的query_set整合到循环外部一个总的Query集合中\n",
    "        print(f\"{unit_num * batch_num} : 生成query完成\")\n",
    "        # 更新Device, 从第二批数据开始，按照query更新\n",
    "\n",
    "        if start_time != 0:\n",
    "            # 记录开始时间\n",
    "            ratioCut_start_time = time_module.time()\n",
    "            for query in Query_sets[-1]:\n",
    "                queried_nodes = [node for node, attributes in G.nodes(data=True) if attributes['device_id'] >= query[1][0] and attributes['device_id'] <= query[1][1]]\n",
    "                for u, v, attrs in G.edges(data=True):\n",
    "                    if u in queried_nodes and v in queried_nodes:\n",
    "                        attrs['weight'] = alpha * G.edges[u, v]['weight'] + beta * G.edges[v, u]['weight']\n",
    "                    else:\n",
    "                        attrs['weight'] = alpha * G.edges[u, v]['weight']\n",
    "\n",
    "        print(f\"{unit_num * batch_num} : 迭代图完成\")\n",
    "        # 谱聚类\n",
    "        Adjacent = nx.adjacency_matrix(G)  # 获取邻接矩阵\n",
    "        total_sum = np.sum(Adjacent.data)  # 对稀疏矩阵中的每个非零元素进行标准化\n",
    "        normalized_adjacent = Adjacent / total_sum  # normalized_adjacent = normalized_adjacent * 1e6\n",
    "        # print(\"包含 NaN:\", np.isnan(normalized_adjacent).any())\n",
    "        Laplacian = calLaplacianMatrix(normalized_adjacent)\n",
    "        Laplacian = Laplacian.astype(np.float64)\n",
    "        # print(\"包含 NaN:\", np.isnan(Laplacian).any())\n",
    "        Laplacian = np.nan_to_num(Laplacian, nan=0.0)\n",
    "\n",
    "        x, V = np.linalg.eig(Laplacian)\n",
    "        x = zip(x, range(len(x)))\n",
    "        x = sorted(x, key=lambda x: x[0])\n",
    "        H = np.vstack([V[:, i] for (v, i) in x[:device_value_max]]).T\n",
    "        class_number = 10\n",
    "        # class_number = int(time_on_chain * device_value_max / unit_num / batch_num) # 类别数量 4\n",
    "        sp_kmeans = KMeans(n_clusters=class_number, n_init='auto').fit(H)\n",
    "        # sp_kmeans = KMeans(n_clusters=16, n_init='auto').fit(H)\n",
    "        labels = sp_kmeans.labels_  # labels 标记了每一个 device 所属的类别\n",
    "        nlist = list(G)  # 20 个 device\n",
    "        print(f\"{unit_num * batch_num} : 谱聚类完成\")\n",
    "        if start_time != 0:\n",
    "            # 记录结束时间\n",
    "            ratioCut_end_time = time_module.time()\n",
    "            # 计算运行时间\n",
    "            execution_time = ratioCut_end_time - ratioCut_start_time\n",
    "            execution_time_ms = execution_time * 1000\n",
    "            exec_time_list.append(execution_time_ms)\n",
    "\n",
    "        node2label = {}\n",
    "        for idx in range(len(nlist)):\n",
    "            node2label[str(nlist[idx] + 1)] = labels[idx]\n",
    "\n",
    "        # 统计列表中每个类别的数量\n",
    "        number_counts = Counter(labels)\n",
    "        class_counts = []\n",
    "\n",
    "        # 由于现在只要对unit 组合为 Batch ，而一个 unit 内的 data_point 都属于同一个 Device ，故只需要找到 20个 Device 与 类别 的映射关系\n",
    "        units_with_label = []\n",
    "        for (device_id, unit) in units_list:\n",
    "            label = node2label[str(device_id)]\n",
    "            units_with_label.append((label, copy.deepcopy(unit)))\n",
    "\n",
    "        # 将unit组合为Batch\n",
    "        batch_list = []\n",
    "        batch_tmp = []\n",
    "        # 20个 Device 被归为了4类，现在遍历所有的unit , 根据unit中的data point的 device_id 决定其所属的类别 class\n",
    "        batch_cnt = 0\n",
    "        units_with_label.sort(key=lambda x: x[0])  # 按照labels对units进行升序排序\n",
    "        for (label, unit) in units_with_label:\n",
    "            batch_tmp.append(copy.deepcopy(unit))\n",
    "            batch_cnt += 1\n",
    "            if batch_cnt % batch_num == 0:\n",
    "                batch_list.append(copy.deepcopy(batch_tmp))\n",
    "                batch_tmp.clear()\n",
    "\n",
    "        for batch in batch_list:\n",
    "            batched_proposed_method[str(cnt)] = batch\n",
    "            cnt += 1\n",
    "        print(f\"{unit_num * batch_num} : batched_proposed_method完成\")\n",
    "    # 计算出RatioCut所需的平均时间\n",
    "    sum = 0.0\n",
    "    for item in exec_time_list:\n",
    "        sum += item\n",
    "    exec_time_avg = sum / len(exec_time_list)\n",
    "    print(exec_time_avg)\n",
    "    results[str(unit_num * batch_num)].append(exec_time_avg)\n",
    "\n",
    "    # 方法一:\n",
    "\n",
    "    store_node_method_r_d = copy.deepcopy(store_node)\n",
    "    # batched_basic_method\n",
    "    store_node_method_r_d_batched_basic_method = copy.deepcopy(store_node_method_r_d)\n",
    "\n",
    "    for key, batch in batched_basic_method.items():\n",
    "        # 要选取一个store node存储\n",
    "        sorted_nodes_r_d = sorted(store_node_method_r_d_batched_basic_method.items(),\n",
    "                                  key=lambda x: x[1]['score_sd'], reverse=False)\n",
    "        # 从得分前 10 的节点中随机选择一个节点\n",
    "        top_ten_nodes_r_d = sorted_nodes_r_d[:10]\n",
    "\n",
    "        saved_flag = False\n",
    "        for node in top_ten_nodes_r_d:\n",
    "            if saved_flag:\n",
    "                break\n",
    "            get_pro = node[1]['probability']\n",
    "            # 得到命中概率\n",
    "            if probabilistic_true(get_pro):\n",
    "                # 若命中，则存入\n",
    "                store_node_method_r_d_batched_basic_method[node[0]]['batches'].append((key, batch))\n",
    "                saved_flag = True\n",
    "        if not saved_flag:\n",
    "            # 10 个都没命中，则存入 rank 第一的 node 中\n",
    "            store_node_method_r_d_batched_basic_method[top_ten_nodes_r_d[0][0]]['batches'].append((key, batch))\n",
    "\n",
    "    # batched_proposed_method\n",
    "    store_node_method_r_d_batched_proposed_method = copy.deepcopy(store_node_method_r_d)\n",
    "\n",
    "    for key, batch in batched_proposed_method.items():\n",
    "        # 要选取一个store node存储\n",
    "        sorted_nodes = sorted(store_node_method_r_d_batched_proposed_method.items(), key=lambda x: x[1]['score_sd'],\n",
    "                              reverse=False)  # 考虑 Distance 升序，距离小的优先\n",
    "        # 从得分前 10 的节点中随机选择一个节点\n",
    "        top_ten_nodes = sorted_nodes[:10]\n",
    "        saved_flag = False\n",
    "        for node in top_ten_nodes:\n",
    "            if saved_flag:\n",
    "                break\n",
    "            get_pro = node[1]['probability']\n",
    "            # 得到命中概率\n",
    "            if probabilistic_true(get_pro):\n",
    "                # 若命中，则存入\n",
    "                store_node_method_r_d_batched_proposed_method[node[0]]['batches'].append(\n",
    "                    (key, batch))  # 考虑 reputation\n",
    "                saved_flag = True\n",
    "        if not saved_flag:\n",
    "            # 10 个都没命中，则存入 rank 第一的 node 中\n",
    "            store_node_method_r_d_batched_proposed_method[top_ten_nodes[0][0]]['batches'].append((key, batch))\n",
    "\n",
    "    # 遍历Query_sets，在两种store node的存储方式上，访问所有的query，计算得到指标\n",
    "    found_in_basic_batch = []  # [[], [], ..., []] 记录了每个 query 中每个 point 所在的batch_id ， 用于后续去 store\n",
    "    found_in_proposed_batch = []\n",
    "    for query_set in Query_sets:\n",
    "        basic_batches4q = []\n",
    "        proposed_batches4q = []\n",
    "        for query in query_set:\n",
    "            # 对每一个 q, 得到 time 和 device\n",
    "            time_low, time_high, device_low, device_high = query[0][0], query[0][1], query[1][0], query[1][1]\n",
    "            # print(f\"time_low:{time_low}, time_high:{time_high}, device_low:{device_low}, device_high:{device_high}\")\n",
    "            # 找到 device 所在的 batch, 计算出每个 q 需要访问哪些 batch\n",
    "            # 直接遍历搜索空间过大，进行优化: 从time来确定Batch_id范围: 0~19s 内的数据处于 0~3 的 batch 之间\n",
    "            # 由于 同一批Query_set中的元素都在同一批Batch中，故直接由time_low定出 batch range\n",
    "            batch_low, batch_high = int(time_low / time_on_chain) * int(\n",
    "                time_on_chain * device_value_max / unit_num / batch_num), (int(time_low / time_on_chain) + 1) * int(\n",
    "                time_on_chain * device_value_max / unit_num / batch_num) - 1\n",
    "            # print(f\"batch_low:{batch_low} and batch_high:{batch_high}\")\n",
    "            for device in range(device_low, device_high + 1):\n",
    "                for time in range(time_low, time_high + 1):\n",
    "                    # print(f\"time: {time}, device: {device}\")\n",
    "                    basic_flag = False\n",
    "                    proposed_flag = False\n",
    "                    # 对每个 （time, device） 找到所属的Batch\n",
    "                    for batch_id in range(batch_low, batch_high + 1):\n",
    "                        # print(f\"batch id is : {batch}\")\n",
    "                        # batch结构: [[], [], ..., []]\n",
    "\n",
    "                        if not basic_flag:\n",
    "                            batch = copy.deepcopy(batched_basic_method[str(batch_id)])\n",
    "                            for unit in batch:\n",
    "                                for point in unit:\n",
    "                                    # print(f\"{point[0]}  and  {point[1]}\")\n",
    "                                    if time == point[0] and device == point[1]:\n",
    "                                        # print(f\"----basic find point\")\n",
    "                                        # 找到了所属的Batch，继续寻找所属的store node\n",
    "                                        basic_batches4q.append(batch_id)\n",
    "                                        basic_flag = True\n",
    "\n",
    "                        if not proposed_flag:\n",
    "                            batch = copy.deepcopy(batched_proposed_method[str(batch_id)])\n",
    "                            for unit in batch:\n",
    "                                for point in unit:\n",
    "                                    if time == point[0] and device == point[1]:\n",
    "                                        # print(f\"----proposed find point\")\n",
    "                                        # 找到了所属的Batch，继续寻找所属的store node\n",
    "                                        proposed_batches4q.append(batch_id)\n",
    "                                        proposed_flag = True\n",
    "                        if proposed_flag and basic_flag:\n",
    "                            break\n",
    "                    # print(f\"belong_to_basic_batch:{belong_to_basic_batch} and belong_to_proposed_batch:{belong_to_proposed_batch}\")\n",
    "            found_in_basic_batch.append(copy.deepcopy(basic_batches4q))\n",
    "            found_in_proposed_batch.append(copy.deepcopy(proposed_batches4q))\n",
    "            basic_batches4q.clear()\n",
    "            proposed_batches4q.clear()\n",
    "\n",
    "    for idx in range(len(found_in_basic_batch)):\n",
    "        found_in_basic_batch[idx] = list(set(found_in_basic_batch[idx]))\n",
    "\n",
    "    for idx in range(len(found_in_proposed_batch)):\n",
    "        found_in_proposed_batch[idx] = list(set(found_in_proposed_batch[idx]))\n",
    "\n",
    "    batches_sum_basic = 0\n",
    "    for item in found_in_basic_batch:\n",
    "        batches_sum_basic += len(item)\n",
    "    print(f\"basic 的 batch 数量{batches_sum_basic}\")\n",
    "\n",
    "    batches_sum_proposed = 0\n",
    "    for item in found_in_proposed_batch:\n",
    "        batches_sum_proposed += len(item)\n",
    "    print(f\"proposed 的 batch 数量{batches_sum_proposed}\")\n",
    "\n",
    "    results[str(unit_num * batch_num)].append((batches_sum_basic, batches_sum_proposed))\n",
    "\n",
    "    # found_in_basic_node\n",
    "    found_in_basic_node_r_d = []\n",
    "    basic_node_tmp = []\n",
    "    for query in found_in_basic_batch:\n",
    "        # print(query)\n",
    "        found_batches = list(set(query))\n",
    "        # print(found_batches)\n",
    "        for batch_id in found_batches:\n",
    "            # 首先要合并所有相同的 batch_id , 再对每个不同的Batch_id到store_node中找到 node_id 存入found_in_xxx_node\n",
    "            flag = False\n",
    "            for node_id, node_info in store_node_method_r_d_batched_basic_method.items():\n",
    "                if flag:\n",
    "                    break\n",
    "                for key, value in node_info['batches']:\n",
    "                    if int(batch_id) == int(key):\n",
    "                        basic_node_tmp.append(node_id)\n",
    "                        flag = True\n",
    "                        break\n",
    "        found_in_basic_node_r_d.append(copy.deepcopy(basic_node_tmp))\n",
    "        basic_node_tmp.clear()\n",
    "\n",
    "    # found_in_proposed_node\n",
    "    found_in_proposed_node_r_d = []\n",
    "    proposed_node_tmp = []\n",
    "    for query in found_in_proposed_batch:\n",
    "        # print(query)\n",
    "        found_batches = list(set(query))\n",
    "        # print(found_batches)\n",
    "        for batch_id in found_batches:\n",
    "            # 首先要合并所有相同的 batch_id , 再对每个不同的Batch_id到store_node中找到 node_id 存入found_in_xxx_node\n",
    "            flag = False\n",
    "            for node_id, node_info in store_node_method_r_d_batched_proposed_method.items():\n",
    "                if flag:\n",
    "                    break\n",
    "                for key, value in node_info['batches']:\n",
    "                    if int(batch_id) == int(key):\n",
    "                        proposed_node_tmp.append(node_id)\n",
    "                        flag = True\n",
    "                        break\n",
    "        found_in_proposed_node_r_d.append(copy.deepcopy(proposed_node_tmp))\n",
    "        proposed_node_tmp.clear()\n",
    "\n",
    "    for idx in range(len(found_in_basic_node_r_d)):\n",
    "        found_in_basic_node_r_d[idx] = list(set(found_in_basic_node_r_d[idx]))\n",
    "    for idx in range(len(found_in_proposed_node_r_d)):\n",
    "        found_in_proposed_node_r_d[idx] = list(set(found_in_proposed_node_r_d[idx]))\n",
    "\n",
    "    nodes_sum_basic = 0\n",
    "    for item in found_in_basic_node_r_d:\n",
    "        nodes_sum_basic += len(item)\n",
    "    print(f\"basic r_d 的 node 数量{nodes_sum_basic}\")\n",
    "    nodes_sum_proposed = 0\n",
    "    for item in found_in_proposed_node_r_d:\n",
    "        nodes_sum_proposed += len(item)\n",
    "    print(f\"proposed r_d 的 node 数量{nodes_sum_proposed}\")\n",
    "    results[str(unit_num * batch_num)].append((nodes_sum_basic, nodes_sum_proposed))\n",
    "\n",
    "    time_consumed_max_basic, refuse_cnt_basic, accept_cnt_basic, serve_prob_basic = cal_time(\n",
    "        found_in_basic_node_r_d, store_node_method_r_d_batched_basic_method)\n",
    "    time_consumed_max_proposed, refuse_cnt_proposed, accept_cnt_proposed, serve_prob_proposed = cal_time(\n",
    "        found_in_proposed_node_r_d, store_node_method_r_d_batched_proposed_method)\n",
    "    results[str(unit_num * batch_num)].append((time_consumed_max_basic, refuse_cnt_basic, accept_cnt_basic, serve_prob_basic))\n",
    "    results[str(unit_num * batch_num)].append((time_consumed_max_proposed, refuse_cnt_proposed, accept_cnt_proposed, serve_prob_proposed))\n",
    "\n",
    "    # 方法二:\n",
    "    store_node_method_r = copy.deepcopy(store_node)\n",
    "    store_node_method_r_batched_basic_method = copy.deepcopy(store_node_method_r)\n",
    "\n",
    "    for key, batch in batched_basic_method.items():\n",
    "        # 要选取一个store node存储\n",
    "        sorted_nodes_r = sorted(store_node_method_r_batched_basic_method.items(), key=lambda x: x[1]['score_sp'],\n",
    "                                reverse=True)  # Ture 指定为降序排序\n",
    "\n",
    "        # 从得分前 10 的节点中随机选择一个节点\n",
    "        top_ten_nodes_r = sorted_nodes_r[:10]\n",
    "        saved_flag = False\n",
    "        for node in top_ten_nodes_r:\n",
    "            if saved_flag:\n",
    "                break\n",
    "            get_pro = node[1]['probability']\n",
    "            # 得到命中概率\n",
    "            if probabilistic_true(get_pro):\n",
    "                # 若命中，则存入\n",
    "                store_node_method_r_batched_basic_method[node[0]]['batches'].append((key, batch))\n",
    "                saved_flag = True\n",
    "\n",
    "        if not saved_flag:\n",
    "            # 10 个都没命中，则存入 rank 第一的 node 中\n",
    "            store_node_method_r_batched_basic_method[top_ten_nodes_r[0][0]]['batches'].append((key, batch))\n",
    "\n",
    "    store_node_method_r_batched_proposed_method = copy.deepcopy(store_node_method_r)\n",
    "\n",
    "    for key, batch in batched_proposed_method.items():\n",
    "        # 要选取一个store node存储\n",
    "        sorted_nodes_r = sorted(store_node_method_r_batched_proposed_method.items(), key=lambda x: x[1]['score_sp'],\n",
    "                                reverse=True)\n",
    "        # 从得分前 10 的节点中随机选择一个节点\n",
    "        top_ten_nodes_r = sorted_nodes_r[:10]\n",
    "        saved_flag = False\n",
    "        for node in top_ten_nodes_r:\n",
    "            if saved_flag:\n",
    "                break\n",
    "            get_pro = node[1]['probability']\n",
    "            # 得到命中概率\n",
    "            if probabilistic_true(get_pro):\n",
    "                # 若命中，则存入\n",
    "                store_node_method_r_batched_proposed_method[node[0]]['batches'].append((key, batch))\n",
    "                saved_flag = True\n",
    "        if not saved_flag:\n",
    "            # 10 个都没命中，则存入 rank 第一的 node 中\n",
    "            store_node_method_r_batched_proposed_method[top_ten_nodes_r[0][0]]['batches'].append((key, batch))\n",
    "\n",
    "    # 遍历Query_sets，在两种store node的存储方式上，访问所有的query，计算得到指标\n",
    "    found_in_basic_batch_r = []  # [[], [], ..., []] 记录了每个 query 中每个 point 所在的batch_id ， 用于后续去 store\n",
    "    found_in_proposed_batch_r = []\n",
    "    for query_set in Query_sets:\n",
    "        basic_batches4q = []\n",
    "        proposed_batches4q = []\n",
    "        for query in query_set:\n",
    "            # 对每一个 q, 得到 time 和 device\n",
    "            time_low, time_high, device_low, device_high = query[0][0], query[0][1], query[1][0], query[1][1]\n",
    "            # print(f\"time_low:{time_low}, time_high:{time_high}, device_low:{device_low}, device_high:{device_high}\")\n",
    "            # 找到 device 所在的 batch, 计算出每个 q 需要访问哪些 batch\n",
    "            # 直接遍历搜索空间过大，进行优化: 从time来确定Batch_id范围: 0~19s 内的数据处于 0~3 的 batch 之间\n",
    "            # 由于 同一批Query_set中的元素都在同一批Batch中，故直接由time_low定出 batch range\n",
    "            batch_low, batch_high = int(time_low / time_on_chain) * int(\n",
    "                time_on_chain * device_value_max / unit_num / batch_num), (int(time_low / time_on_chain) + 1) * int(\n",
    "                time_on_chain * device_value_max / unit_num / batch_num) - 1\n",
    "            # print(f\"batch_low:{batch_low} and batch_high:{batch_high}\")\n",
    "            for device in range(device_low, device_high + 1):\n",
    "                for time in range(time_low, time_high + 1):\n",
    "                    # print(f\"time: {time}, device: {device}\")\n",
    "                    basic_flag = False\n",
    "                    proposed_flag = False\n",
    "                    # 对每个 （time, device） 找到所属的Batch\n",
    "                    for batch_id in range(batch_low, batch_high + 1):\n",
    "                        # print(f\"batch id is : {batch}\")\n",
    "                        # batch结构: [[], [], ..., []]\n",
    "                        if not basic_flag:\n",
    "                            batch = copy.deepcopy(batched_basic_method[str(batch_id)])\n",
    "                            for unit in batch:\n",
    "                                for point in unit:\n",
    "                                    # print(f\"{point[0]}  and  {point[1]}\")\n",
    "                                    if time == point[0] and device == point[1]:\n",
    "                                        # print(f\"----basic find point\")\n",
    "                                        # 找到了所属的Batch，继续寻找所属的store node\n",
    "                                        basic_batches4q.append(batch_id)\n",
    "                                        basic_flag = True\n",
    "                        if not proposed_flag:\n",
    "                            batch = copy.deepcopy(batched_proposed_method[str(batch_id)])\n",
    "                            for unit in batch:\n",
    "                                for point in unit:\n",
    "                                    if time == point[0] and device == point[1]:\n",
    "                                        # print(f\"----proposed find point\")\n",
    "                                        # 找到了所属的Batch，继续寻找所属的store node\n",
    "                                        proposed_batches4q.append(batch_id)\n",
    "                                        proposed_flag = True\n",
    "                        if proposed_flag and basic_flag:\n",
    "                            break\n",
    "                    # print(f\"belong_to_basic_batch:{belong_to_basic_batch} and belong_to_proposed_batch:{belong_to_proposed_batch}\")\n",
    "            found_in_basic_batch_r.append(copy.deepcopy(basic_batches4q))\n",
    "            found_in_proposed_batch_r.append(copy.deepcopy(proposed_batches4q))\n",
    "            basic_batches4q.clear()\n",
    "            proposed_batches4q.clear()\n",
    "\n",
    "    for idx in range(len(found_in_basic_batch_r)):\n",
    "        found_in_basic_batch_r[idx] = list(set(found_in_basic_batch_r[idx]))\n",
    "\n",
    "    for idx in range(len(found_in_proposed_batch_r)):\n",
    "        found_in_proposed_batch_r[idx] = list(set(found_in_proposed_batch_r[idx]))\n",
    "\n",
    "    found_in_basic_node_r = []\n",
    "    basic_node_tmp = []\n",
    "    for query in found_in_basic_batch_r:\n",
    "        # print(query)\n",
    "        found_batches = list(set(query))\n",
    "        # print(found_batches)\n",
    "        for batch_id in found_batches:\n",
    "            # 首先要合并所有相同的 batch_id , 再对每个不同的Batch_id到store_node中找到 node_id 存入found_in_xxx_node\n",
    "            flag = False\n",
    "            for node_id, node_info in store_node_method_r_batched_basic_method.items():\n",
    "                if flag:\n",
    "                    break\n",
    "                for key, value in node_info['batches']:\n",
    "                    if int(batch_id) == int(key):\n",
    "                        basic_node_tmp.append(node_id)\n",
    "                        flag = True\n",
    "                        break\n",
    "        found_in_basic_node_r.append(copy.deepcopy(basic_node_tmp))\n",
    "        basic_node_tmp.clear()\n",
    "\n",
    "    # found_in_proposed_node\n",
    "    found_in_proposed_node_r = []\n",
    "    proposed_node_tmp = []\n",
    "    for query in found_in_proposed_batch:\n",
    "        # print(query)\n",
    "        found_batches = list(set(query))\n",
    "        # print(found_batches)\n",
    "        for batch_id in found_batches:\n",
    "            # 首先要合并所有相同的 batch_id , 再对每个不同的Batch_id到store_node中找到 node_id 存入found_in_xxx_node\n",
    "            flag = False\n",
    "            for node_id, node_info in store_node_method_r_batched_proposed_method.items():\n",
    "                if flag:\n",
    "                    break\n",
    "                for key, value in node_info['batches']:\n",
    "                    if int(batch_id) == int(key):\n",
    "                        proposed_node_tmp.append(node_id)\n",
    "                        flag = True\n",
    "                        break\n",
    "        found_in_proposed_node_r.append(copy.deepcopy(proposed_node_tmp))\n",
    "        proposed_node_tmp.clear()\n",
    "\n",
    "    for idx in range(len(found_in_basic_node_r)):\n",
    "        found_in_basic_node_r[idx] = list(set(found_in_basic_node_r[idx]))\n",
    "\n",
    "    for idx in range(len(found_in_proposed_node_r)):\n",
    "        found_in_proposed_node_r[idx] = list(set(found_in_proposed_node_r[idx]))\n",
    "\n",
    "    nodes_sum_basic = 0\n",
    "    for item in found_in_basic_node_r:\n",
    "        nodes_sum_basic += len(item)\n",
    "    print(f\"basic r 的 node 数量{nodes_sum_basic}\")\n",
    "    nodes_sum_proposed = 0\n",
    "    for item in found_in_proposed_node_r:\n",
    "        nodes_sum_proposed += len(item)\n",
    "    print(f\"proposed r 的 node 数量{nodes_sum_proposed}\")\n",
    "\n",
    "    results[str(unit_num * batch_num)].append((nodes_sum_basic, nodes_sum_proposed))\n",
    "\n",
    "    time_consumed_max_basic, refuse_cnt_basic, accept_cnt_basic, serve_prob_basic = cal_time(found_in_basic_node_r, store_node_method_r_batched_basic_method)\n",
    "    time_consumed_max_proposed, refuse_cnt_proposed, accept_cnt_proposed, serve_prob_proposed = cal_time(found_in_proposed_node_r, store_node_method_r_batched_proposed_method)\n",
    "\n",
    "    results[str(unit_num * batch_num)].append((time_consumed_max_basic, refuse_cnt_basic, accept_cnt_basic, serve_prob_basic))\n",
    "    results[str(unit_num * batch_num)].append((time_consumed_max_proposed, refuse_cnt_proposed, accept_cnt_proposed, serve_prob_proposed))\n",
    "\n",
    "\n",
    "    # 方法三\n",
    "    store_node_method_d = copy.deepcopy(store_node)\n",
    "\n",
    "    # batched_basic_method\n",
    "    store_node_method_d_batched_basic_method = copy.deepcopy(store_node_method_d)\n",
    "\n",
    "    for key, batch in batched_basic_method.items():\n",
    "        # 要选取一个store node存储\n",
    "        sorted_nodes_d = sorted(store_node_method_d_batched_basic_method.items(), key=lambda x: x[1]['score_sd'],\n",
    "                                reverse=False)  # 升序排序，距离越小越优先\n",
    "        # 从得分前 10 的节点中随机选择一个节点\n",
    "        top_ten_nodes = sorted_nodes_d[:10]\n",
    "        random_choice_node = random.choice(top_ten_nodes)\n",
    "        store_node_method_d_batched_basic_method[random_choice_node[0]]['batches'].append((key, batch))\n",
    "\n",
    "    store_node_method_d_batched_proposed_method = copy.deepcopy(store_node_method_d)\n",
    "\n",
    "    for key, batch in batched_proposed_method.items():\n",
    "        # 要选取一个store node存储\n",
    "        sorted_nodes_d = sorted(store_node_method_d_batched_proposed_method.items(), key=lambda x: x[1]['score_sd'],\n",
    "                                reverse=False)  # 升序排序，距离越小越优先\n",
    "        # 从得分前 10 的节点中随机选择一个节点\n",
    "        top_ten_nodes = sorted_nodes_d[:10]\n",
    "        random_choice_node = random.choice(top_ten_nodes)\n",
    "        store_node_method_d_batched_proposed_method[random_choice_node[0]]['batches'].append((key, batch))\n",
    "\n",
    "    # 遍历Query_sets，在两种store node的存储方式上，访问所有的query，计算得到指标\n",
    "    found_in_basic_batch = []  # [[], [], ..., []] 记录了每个 query 中每个 point 所在的batch_id ， 用于后续去 store\n",
    "    found_in_proposed_batch = []\n",
    "    found_in_basic_batch.clear()\n",
    "    found_in_proposed_batch.clear()\n",
    "    for query_set in Query_sets:\n",
    "        basic_batches4q = []\n",
    "        proposed_batches4q = []\n",
    "        for query in query_set:\n",
    "            # 对每一个 q, 得到 time 和 device\n",
    "            time_low, time_high, device_low, device_high = query[0][0], query[0][1], query[1][0], query[1][1]\n",
    "            # print(f\"time_low:{time_low}, time_high:{time_high}, device_low:{device_low}, device_high:{device_high}\")\n",
    "            # 找到 device 所在的 batch, 计算出每个 q 需要访问哪些 batch\n",
    "            # 直接遍历搜索空间过大，进行优化: 从time来确定Batch_id范围: 0~19s 内的数据处于 0~3 的 batch 之间\n",
    "            # 由于 同一批Query_set中的元素都在同一批Batch中，故直接由time_low定出 batch range\n",
    "            batch_low, batch_high = int(time_low / time_on_chain) * int(\n",
    "                time_on_chain * device_value_max / unit_num / batch_num), (int(time_low / time_on_chain) + 1) * int(\n",
    "                time_on_chain * device_value_max / unit_num / batch_num) - 1\n",
    "            # print(f\"batch_low:{batch_low} and batch_high:{batch_high}\")\n",
    "            for device in range(device_low, device_high + 1):\n",
    "                for time in range(time_low, time_high + 1):\n",
    "                    # print(f\"time: {time}, device: {device}\")\n",
    "                    basic_flag = False\n",
    "                    proposed_flag = False\n",
    "                    # 对每个 （time, device） 找到所属的Batch\n",
    "                    for batch_id in range(batch_low, batch_high + 1):\n",
    "                        # print(f\"batch id is : {batch}\")\n",
    "                        # batch结构: [[], [], ..., []]\n",
    "\n",
    "                        if not basic_flag:\n",
    "                            batch = copy.deepcopy(batched_basic_method[str(batch_id)])\n",
    "                            for unit in batch:\n",
    "                                for point in unit:\n",
    "                                    # print(f\"{point[0]}  and  {point[1]}\")\n",
    "                                    if time == point[0] and device == point[1]:\n",
    "                                        # print(f\"----basic find point\")\n",
    "                                        # 找到了所属的Batch，继续寻找所属的store node\n",
    "                                        basic_batches4q.append(batch_id)\n",
    "                                        basic_flag = True\n",
    "\n",
    "                        if not proposed_flag:\n",
    "                            batch = copy.deepcopy(batched_proposed_method[str(batch_id)])\n",
    "                            for unit in batch:\n",
    "                                for point in unit:\n",
    "                                    if time == point[0] and device == point[1]:\n",
    "                                        # print(f\"----proposed find point\")\n",
    "                                        # 找到了所属的Batch，继续寻找所属的store node\n",
    "                                        proposed_batches4q.append(batch_id)\n",
    "                                        proposed_flag = True\n",
    "                        if proposed_flag and basic_flag:\n",
    "                            break\n",
    "                    # print(f\"belong_to_basic_batch:{belong_to_basic_batch} and belong_to_proposed_batch:{belong_to_proposed_batch}\")\n",
    "            found_in_basic_batch.append(copy.deepcopy(basic_batches4q))\n",
    "            found_in_proposed_batch.append(copy.deepcopy(proposed_batches4q))\n",
    "            basic_batches4q.clear()\n",
    "            proposed_batches4q.clear()\n",
    "\n",
    "    for idx in range(len(found_in_basic_batch)):\n",
    "        found_in_basic_batch[idx] = list(set(found_in_basic_batch[idx]))\n",
    "    for idx in range(len(found_in_proposed_batch)):\n",
    "        found_in_proposed_batch[idx] = list(set(found_in_proposed_batch[idx]))\n",
    "\n",
    "    found_in_basic_node_d = []\n",
    "    basic_node_tmp = []\n",
    "    for query in found_in_basic_batch:\n",
    "        # print(query)\n",
    "        found_batches = list(set(query))\n",
    "        # print(found_batches)\n",
    "        for batch_id in found_batches:\n",
    "            # 首先要合并所有相同的 batch_id , 再对每个不同的Batch_id到store_node中找到 node_id 存入found_in_xxx_node\n",
    "            flag = False\n",
    "            for node_id, node_info in store_node_method_d_batched_basic_method.items():\n",
    "                if flag:\n",
    "                    break\n",
    "                for key, value in node_info['batches']:\n",
    "                    if int(batch_id) == int(key):\n",
    "                        basic_node_tmp.append(node_id)\n",
    "                        flag = True\n",
    "                        break\n",
    "        found_in_basic_node_d.append(copy.deepcopy(basic_node_tmp))\n",
    "        basic_node_tmp.clear()\n",
    "\n",
    "    # found_in_proposed_node\n",
    "    found_in_proposed_node_d = []\n",
    "    proposed_node_tmp = []\n",
    "    for query in found_in_proposed_batch:\n",
    "        # print(query)\n",
    "        found_batches = list(set(query))\n",
    "        # print(found_batches)\n",
    "        for batch_id in found_batches:\n",
    "            # 首先要合并所有相同的 batch_id , 再对每个不同的Batch_id到store_node中找到 node_id 存入found_in_xxx_node\n",
    "            flag = False\n",
    "            for node_id, node_info in store_node_method_d_batched_proposed_method.items():\n",
    "                if flag:\n",
    "                    break\n",
    "                for key, value in node_info['batches']:\n",
    "                    if int(batch_id) == int(key):\n",
    "                        proposed_node_tmp.append(node_id)\n",
    "                        flag = True\n",
    "                        break\n",
    "        found_in_proposed_node_d.append(copy.deepcopy(proposed_node_tmp))\n",
    "        proposed_node_tmp.clear()\n",
    "\n",
    "    for idx in range(len(found_in_basic_node_d)):\n",
    "        found_in_basic_node_d[idx] = list(set(found_in_basic_node_d[idx]))\n",
    "    for idx in range(len(found_in_proposed_node_d)):\n",
    "        found_in_proposed_node_d[idx] = list(set(found_in_proposed_node_d[idx]))\n",
    "\n",
    "    nodes_sum_basic = 0\n",
    "    for item in found_in_basic_node_d:\n",
    "        nodes_sum_basic += len(item)\n",
    "    print(f\"basic r 的 node 数量{nodes_sum_basic}\")\n",
    "    nodes_sum_proposed = 0\n",
    "    for item in found_in_proposed_node_d:\n",
    "        nodes_sum_proposed += len(item)\n",
    "    print(f\"proposed r 的 node 数量{nodes_sum_proposed}\")\n",
    "\n",
    "    results[str(unit_num * batch_num)].append((nodes_sum_basic, nodes_sum_proposed))\n",
    "\n",
    "    time_consumed_max_basic, refuse_cnt_basic, accept_cnt_basic, serve_prob_basic = (\n",
    "        cal_time(found_in_basic_node_d, store_node_method_d_batched_basic_method))\n",
    "    time_consumed_max_proposed, refuse_cnt_proposed, accept_cnt_proposed, serve_prob_proposed =\\\n",
    "        cal_time(found_in_proposed_node_d, store_node_method_d_batched_proposed_method)\n",
    "\n",
    "    results[str(unit_num * batch_num)].append(\n",
    "        (time_consumed_max_basic, refuse_cnt_basic, accept_cnt_basic, serve_prob_basic))\n",
    "    results[str(unit_num * batch_num)].append(\n",
    "        (time_consumed_max_proposed, refuse_cnt_proposed, accept_cnt_proposed, serve_prob_proposed))\n",
    "\n",
    "    result_temp = {}\n",
    "    for key, value in results.items():\n",
    "        if len(results[key]) != 0:\n",
    "            result_temp[key] = value\n",
    "    df = pd.DataFrame(result_temp)\n",
    "    # 将DataFrame写入到Excel文件\n",
    "    output_file = f'./output_batch_size_{unit_num * batch_num}.xlsx'\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(f'Data has been written to {output_file}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T03:49:07.809499Z",
     "start_time": "2024-07-02T02:02:29.170124Z"
    }
   },
   "id": "9603937d8e80409c",
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "194410a700447ba2",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
